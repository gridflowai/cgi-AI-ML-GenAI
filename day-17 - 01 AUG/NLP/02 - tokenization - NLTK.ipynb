{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "951c2a40-5f6d-47ae-8a62-54baf64a6ba8",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "#### Tokenization\n",
    "- NLTK (Natural Language Toolkit) provides various tokenization methods to suit different use cases and requirements in Natural Language Processing (NLP). \n",
    "- Here are examples of different types of tokenization supported by NLTK\n",
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224e4c57-f9b7-4144-a7ec-6c6a5ad722d7",
   "metadata": {},
   "source": [
    "#### What is Tokenization?\n",
    "A token is a piece of a whole, so a word is a token in a sentence, and a sentence is a token in a paragraph. Tokenization is the process of splitting a string into a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b3b3bd99-f5f4-40b4-84cd-7a1c43fb8216",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4980eaf3-2e77-474b-9c34-ecf517855681",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My', 'favorite', 'color', 'is', 'blue']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystring = \"My favorite color is blue\"\n",
    "\n",
    "mystring.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "71618bf8-9b9c-485e-975c-b8d85194e496",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mystring = \"My favorite colors are blue, red, and green.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e445013b-9c29-49df-83d3-cdd4794231db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My', 'favorite', 'colors', 'are', 'blue,', 'red,', 'and', 'green.']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystring.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0da0ab1-e0e2-4e4d-a4df-cb18fa687960",
   "metadata": {},
   "source": [
    "the punctuation marks are grouped in with their adjacent word (e.g. blue,). This is problematic for NLP applications, as the goal of tokenization is generally to divide a set (corpus) of documents into a common set of building blocks that can then be used as a basis for comparison. Hence, it‚Äôs no good if ‚Äúblue‚Äù in \"My favorite color is blue\" doesn‚Äôt match with ‚Äúblue‚Äù in \"My favorite colors are blue, red, and green.\" since the latter is tokenized as blue, rather than blue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a4181c-c9d7-4f0b-85d9-5acc595b6a92",
   "metadata": {},
   "source": [
    "#### Word Tokenization (Word Tokenizer)\n",
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b8a5a6a2-5674-471d-b3d4-831bc7d7d519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# has some grammer rules \n",
    "# word_tokenize is more than split()\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bb18f1fd-75b6-441d-a0cb-7e9ae06c8ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLTK', 'provides', 'word', 'tokenization', 'capabilities', '.']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text  = \"NLTK provides word tokenization capabilities.\"\n",
    "words = word_tokenize(text)\n",
    "\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf8e8a2-8eff-4a42-8104-b3bca546e9c6",
   "metadata": {},
   "source": [
    "Pros:\n",
    "- Straightforward and widely applicable\n",
    "- Handles most common cases well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2762403-d754-4381-9e4f-fa47b8cb9d04",
   "metadata": {},
   "source": [
    "Cons:\n",
    "- May not handle domain-specific or noisy text effectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9e5176d1-da4c-415c-94c7-baff85c12ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokens: ['DXC', 'The', 'protein-protein', 'interaction', 'network', 'is', 'abc-xyz', 'essential', 'for', 'understanding', 'biological', 'processes', '.']\n"
     ]
    }
   ],
   "source": [
    "# Text containing domain-specific terminology\n",
    "text = \"DXC The protein-protein interaction network is abc-xyz essential for understanding biological processes.\"\n",
    "\n",
    "# Tokenize the text\n",
    "words = word_tokenize(text)\n",
    "\n",
    "print(\"Word Tokens:\", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b574dfd-617a-4807-8d17-d25bf877ba17",
   "metadata": {},
   "source": [
    "recognizes 'protein-protein' as one word which is good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0d9f9259-4b6a-4137-a56e-371bbe3a88ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokens: ['Dr.', 'Smith', 'received', 'his', 'Ph.D.', 'from', 'NYU', '.']\n"
     ]
    }
   ],
   "source": [
    "# Text containing abbreviations with periods\n",
    "text = \"Dr. Smith received his Ph.D. from NYU.\"\n",
    "\n",
    "# Tokenize the text\n",
    "words = word_tokenize(text)\n",
    "\n",
    "print(\"Word Tokens:\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4bd99e99-a777-47c0-8521-f680519c415c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokens: ['This', 'is', 'a', 'sentence', '..', 'with', '..', 'multiple', '..', 'periods', '...']\n"
     ]
    }
   ],
   "source": [
    "# Text with unconventional punctuation\n",
    "text = \"This is a sentence..with..multiple..periods...\"\n",
    "\n",
    "# Tokenize the text\n",
    "words = word_tokenize(text)\n",
    "\n",
    "print(\"Word Tokens:\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7fcefe7c-50bc-4c5b-8b60-5ad1f165c790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokens: ['I', \"'m\", 'feeling', 'üòä', 'today', '!', '#', 'happy']\n"
     ]
    }
   ],
   "source": [
    "# Text containing special characters and emoticons\n",
    "text = \"I'm feeling üòä today! #happy\"\n",
    "\n",
    "# Tokenize the text\n",
    "words = word_tokenize(text)\n",
    "\n",
    "print(\"Word Tokens:\", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538f726a-5d6b-4d4d-b65a-556a49f0eea2",
   "metadata": {},
   "source": [
    "NLTK's word_tokenize may not handle special characters and emoticons gracefully, splitting them into separate tokens instead of treating them as part of a single token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5111b5e8-455e-45bc-9771-8386953ba904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokens: ['I', 'do', \"n't\", 'know', 'what', 'I', \"'d\", 'do', 'without', 'NLTK', '.']\n"
     ]
    }
   ],
   "source": [
    "# Text containing contractions\n",
    "text = \"I don't know what I'd do without NLTK.\"\n",
    "\n",
    "# Tokenize the text\n",
    "words = word_tokenize(text)\n",
    "\n",
    "print(\"Word Tokens:\", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5d025a-92ee-46d6-816c-00b63299d890",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "- NLTK's word_tokenize may split contractions like \"don't\" and \"I'd\" into separate tokens, treating the apostrophe as a delimiter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "090bc3f5-1bff-47ec-a77d-d7b880de9ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you have\n",
      "do not\n"
     ]
    }
   ],
   "source": [
    "import contractions\n",
    "print(contractions.fix(\"you've\"))\n",
    "print(contractions.fix(\"don't\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dc126094-e292-44d9-ac43-2d90fd084092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokens: ['LOL', '!', '!', ',', 'IDK', ',', 'BRB', ',', 'BTW', ',', 'TTYL']\n"
     ]
    }
   ],
   "source": [
    "# Text containing internet slang and abbreviations\n",
    "text = \"LOL!!, IDK, BRB, BTW, TTYL\"\n",
    "\n",
    "# Tokenize the text\n",
    "words = word_tokenize(text)\n",
    "\n",
    "print(\"Word Tokens:\", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84f0fb2-9207-446d-b11b-24a839e624b8",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "- NLTK's word_tokenize may not recognize internet slang and abbreviations as single tokens, splitting them into separate tokens based on punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8d776e61-35d5-483c-81ef-d697f2075087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokens: ['The', 'value', 'of', 'œÄ', 'is', 'approximately', '3.14159', '.']\n"
     ]
    }
   ],
   "source": [
    "# Text containing technical terms and symbols\n",
    "text = \"The value of œÄ is approximately 3.14159.\"\n",
    "\n",
    "# Tokenize the text\n",
    "words = word_tokenize(text)\n",
    "\n",
    "print(\"Word Tokens:\", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6825f7b1-fe9b-43ea-bba7-d35cbf3f62f9",
   "metadata": {},
   "source": [
    "**Use Cases of word_tokenize**\n",
    "- Text classification\n",
    "- Named entity recognition\n",
    "- Part-of-speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c32d1b5-e749-4a6e-a2e7-ef85cafa76d7",
   "metadata": {},
   "source": [
    "--------------------------------------\n",
    "#### Sentence Tokenization (Sentence Tokenizer)\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f309b02c-9a1e-4a52-9fe7-1a972e78f74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a83fe7f4-6e6d-4ad7-9c5d-a83ea8641146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLTK provides sentence tokenization capabilities.',\n",
       " 'This is an example sentence.']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"NLTK provides sentence tokenization capabilities. This is an example sentence.\"\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e6b30a-3505-4034-8210-fa3e0d4c5cd2",
   "metadata": {},
   "source": [
    "**Use Cases**\n",
    "- Text summarization\n",
    "- Machine translation\n",
    "- Text segmentation\n",
    "\n",
    "**Pros**\n",
    "- Useful for segmenting text into meaningful units\n",
    "- Handles various sentence structures and punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392161f7-4a73-49e6-b3c0-1383f70759a9",
   "metadata": {},
   "source": [
    "-------------------------------\n",
    "#### Treebank Tokenizer: \n",
    "-------------------------------------\n",
    "\n",
    "Splits text into words using the Penn Treebank tokenization rules.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d7c937-cf85-491a-81dc-81eacbf4f257",
   "metadata": {},
   "source": [
    "**Key Characteristics** \n",
    "\n",
    "- `Word Segmentation`: The Treebank tokenizer splits text into individual words or tokens based on whitespace characters, punctuation marks, and other language-specific rules.\n",
    "\n",
    "- `Punctuation Handling`: It handles punctuation marks appropriately, considering them as separate tokens when not part of a word.\n",
    "\n",
    "- `Hyphenated Words`: Hyphenated words like \"state-of-the-art\" are treated as single tokens, maintaining their integrity.\n",
    "\n",
    "- `Abbreviations`: It recognizes common abbreviations and acronyms as single tokens, such as \"U.S.A.\" or \"Mr.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ca08ae5b-838e-4492-9700-0af3cd16f3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "679e9b53-3675-4ba8-afb3-3e0f6c8d67eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLTK', \"'s\", 'Treebank', 'tokenizer', 'splits', 'text', 'into', 'words', 'accurately', 'and', 'state-of-the-art']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "text = \"NLTK's Treebank tokenizer splits text into words accurately and state-of-the-art\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16614967-fc0f-4be0-aeb3-197c19dea716",
   "metadata": {},
   "source": [
    "Try the following hyphenated words\n",
    "\n",
    "- Well-known: Familiar or widely recognized by many people.\n",
    "- High-speed: Referring to something that moves or operates at a fast pace.\n",
    "- Mother-in-law: The mother of one's spouse.\n",
    "- Self-esteem: Confidence and satisfaction in oneself.\n",
    "- Run-down: In poor or deteriorated condition.\n",
    "- Second-hand: Previously owned or used by someone else.\n",
    "- Up-to-date: Current or modern, reflecting the latest information or trends.\n",
    "- Co-founder: One of the individuals who jointly establishes a company or organization.\n",
    "- Off-the-shelf: Ready-made or pre-packaged, available for immediate use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "633fcd23-37f6-4d2d-96b0-d35eebebb751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', \"'ve\", ',', 'it', 'ca', \"n't\", 'be', 'done']\n"
     ]
    }
   ],
   "source": [
    "text = \"i've, it can't be done\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b3507c31-7e52-4992-a0c3-c6b44921722a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', \"'s\", 'going', 'to', 'the', 'park', 'tomorrow', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"He's going to the park tomorrow.\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "357fed85-8f22-40a2-9534-7fc52ecd4e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['They', \"'ll\", 'arrive', 'late', 'tonight', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"They'll arrive late tonight.\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "373060c3-6542-4d0a-9646-8d43bee0eda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['They', \"'ll\", 'arrive', 'late', 'tonight', '.']\n"
     ]
    }
   ],
   "source": [
    "ext = \"Can't you see what's happening?\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384f766e-4f61-43d1-a160-3282312ee002",
   "metadata": {},
   "source": [
    "- contractions are split into separate tokens by the TreebankWordTokenizer, treating the apostrophe as a delimiter. (ISSUE)\n",
    "\n",
    "- If you require contractions to be treated as single tokens, you may need to use a different tokenizer or perform additional pre-processing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f147f6-851a-418f-8ae9-8b215d895a75",
   "metadata": {},
   "source": [
    "Pros:\n",
    "- Accurate tokenization based on well-defined syntactic conventions.\n",
    "- Handles a wide range of text formats and styles effectively.\n",
    "- Retains the integrity of hyphenated words and abbreviations.\n",
    "\n",
    "Cons:\n",
    "- May tokenize certain non-standard or domain-specific terms inaccurately.\n",
    "- Requires additional preprocessing for specific tokenization requirements beyond standard English text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb82335d-927e-423e-a43f-9b0aba3feb9f",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "#### Word Punkt Tokenizer \n",
    "-----------------------\n",
    "- splits text into words using a set of rules derived from punctuation. \n",
    "- It is based on the `Penn Treebank tokenizer` but includes additional rules for handling punctuation and contractions more effectively. \n",
    "\n",
    "\n",
    "- The Word Punkt Tokenizer utilizes a `set of regular expressions` to split text into words based on punctuation marks, including periods, commas, hyphens, parentheses, and quotation marks. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e1dc945a-25ca-42bf-9810-fd65da44e906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "17e5f3e2-5532-4688-b431-58e927c36399",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "66cf7270-b08a-49f4-85cd-834056d45aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLTK', \"'\", 's', 'Word', 'Punkt', 'Tokenizer', 'splits', 'text', 'into', 'words', 'effectively', '.', '!', '!', '!', '@@##']\n"
     ]
    }
   ],
   "source": [
    "text = \"NLTK's Word Punkt Tokenizer splits text into words effectively. ! ! ! @@##\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dc72328a-9ecd-4615-bba6-70f9fd36fb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'can', \"'\", 't', 'believe', 'it', '!']\n"
     ]
    }
   ],
   "source": [
    "# Handling Contractions\n",
    "# not good\n",
    "text = \"I can't believe it!\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "da80173b-76ba-43f7-9a44-6c2a685b66b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'\", 'm', 'feeling', 'üòäüòäüòäüòä', 'today', '!']\n"
     ]
    }
   ],
   "source": [
    "text = \"I'm feeling üòäüòäüòäüòä today!\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "488aabb8-6d34-427e-b7ac-3b039caa0f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'graduated', 'from', 'St', '.', 'John', \"'\", 's', 'University', '.']\n"
     ]
    }
   ],
   "source": [
    "# Abbreviations\n",
    "# not good\n",
    "text = \"He graduated from St. John's University.\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "46c77809-d800-405e-a422-38b44983ca17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'high', '-', 'speed', 'fence', '.']\n"
     ]
    }
   ],
   "source": [
    "# Hyphenated Words\n",
    "# not good\n",
    "text = \"The quick brown fox jumped over the high-speed fence.\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0c5d4f-8b03-477b-a115-133cc29e9b0a",
   "metadata": {},
   "source": [
    "Pros:\n",
    "- Effectively tokenizes text based on punctuation marks.\n",
    "- Suitable for basic tokenization tasks in informal text settings.\n",
    "\n",
    "Cons:\n",
    "- Does not handle contractions, abbreviations, or hyphenated words as single tokens.\n",
    "- May split words unnecessarily at punctuation marks, resulting in additional tokens.\n",
    "- Not suitable for handling complex linguistic structures or specific text formats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd96305e-2692-4bd8-8894-2363f1ec9f45",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "#### Space Tokenizer: Tokenizes text based on spaces.\n",
    "---------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb1708f1-6c49-4360-a97c-53bf5c6933e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import SpaceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "20f986d6-c493-44fa-b3cc-8f17fc4325f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SpaceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "db23e469-f254-4658-bf90-f2e2b5402daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['NLTK', 'Space', 'Tokenizer', 'splits', 'text', 'based', 'on', 'spaces.']\n"
     ]
    }
   ],
   "source": [
    "# Example text\n",
    "text = \"NLTK Space Tokenizer splits text based on spaces.\"\n",
    "\n",
    "# Tokenizing the text\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "# Print tokens\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f72206-1f1e-4634-9448-aace00f3656f",
   "metadata": {},
   "source": [
    "Pros:\n",
    "- Simple and lightweight tokenizer.\n",
    "- Efficient for tokenizing text with uniform spacing between words.\n",
    "- Preserves original whitespace characters in the tokens.\n",
    "\n",
    "Cons:\n",
    "- Does not handle punctuation marks, contractions, or other non-whitespace delimiters.\n",
    "- May not be suitable for text with irregular spacing or special characters within words.\n",
    "- Does not handle complex linguistic structures or tokenization tasks beyond basic word splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8240a4f8-852e-4469-b0a4-d399069db64b",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c2742db7-f5da-44b8-9a84-2ded15119486",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compare_list = ['https://t.co/9z2J3P33Uc',\n",
    "               'laugh/cry',\n",
    "               'üò¨üò≠üòìü§¢üôÑüò±',\n",
    "               \"world's problems\",\n",
    "               \"@datageneral\",\n",
    "                \"It's interesting\",\n",
    "               \"don't spell my name right\",\n",
    "               'all-nighter',\n",
    "                \"My favorite color is blue\",\n",
    "                \"My favorite colors are blue, red, and green.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d732a46e-1a49-4d0e-a0d4-6b000d17ae92",
   "metadata": {},
   "source": [
    "**`NLTK word_tokenize`** - separate words using spaces and punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7b3b4051-c133-4a5b-883a-d080538fb821",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['https', ':', '//t.co/9z2J3P33Uc'],\n",
       " ['laugh/cry'],\n",
       " ['üò¨üò≠üòìü§¢üôÑüò±'],\n",
       " ['world', \"'s\", 'problems'],\n",
       " ['@', 'datageneral'],\n",
       " ['It', \"'s\", 'interesting'],\n",
       " ['do', \"n't\", 'spell', 'my', 'name', 'right'],\n",
       " ['all-nighter'],\n",
       " ['My', 'favorite', 'color', 'is', 'blue'],\n",
       " ['My',\n",
       "  'favorite',\n",
       "  'colors',\n",
       "  'are',\n",
       "  'blue',\n",
       "  ',',\n",
       "  'red',\n",
       "  ',',\n",
       "  'and',\n",
       "  'green',\n",
       "  '.']]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokens = []\n",
    "\n",
    "for sent in compare_list:\n",
    "\n",
    "    word_tokens.append(word_tokenize(sent))\n",
    "\n",
    "word_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88739dd-5e56-4cd9-9cc7-7d548a5d66a0",
   "metadata": {},
   "source": [
    "When dealing with well-formed, formal text, this standard word tokenizer makes a lot of sense and is likely to be sufficient. \n",
    "\n",
    "However, the same cannot be said for cases when our text data comes from more casual, slang-ridden sources like Twitter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7688c86-8a6d-4626-9bdc-566adb485b40",
   "metadata": {},
   "source": [
    "**`WordPunctTokenizer`** splits all punctuations into separate tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8bbe9891-12ce-42ba-94ad-cd0828b4ddc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "839f90c7-de6b-4cc0-aa7c-33e8e9e1192e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['https', '://', 't', '.', 'co', '/', '9z2J3P33Uc'],\n",
       " ['laugh', '/', 'cry'],\n",
       " ['üò¨üò≠üòìü§¢üôÑüò±'],\n",
       " ['world', \"'\", 's', 'problems'],\n",
       " ['@', 'datageneral'],\n",
       " ['It', \"'\", 's', 'interesting'],\n",
       " ['don', \"'\", 't', 'spell', 'my', 'name', 'right'],\n",
       " ['all', '-', 'nighter'],\n",
       " ['My', 'favorite', 'color', 'is', 'blue'],\n",
       " ['My',\n",
       "  'favorite',\n",
       "  'colors',\n",
       "  'are',\n",
       "  'blue',\n",
       "  ',',\n",
       "  'red',\n",
       "  ',',\n",
       "  'and',\n",
       "  'green',\n",
       "  '.']]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punct_tokenizer = WordPunctTokenizer()\n",
    "\n",
    "punct_tokens = []\n",
    "\n",
    "for sent in compare_list:\n",
    "    \n",
    "    punct_tokens.append(punct_tokenizer.tokenize(sent))\n",
    "punct_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594ba213-c3b4-4bd3-92d2-56b0337ceccf",
   "metadata": {},
   "source": [
    "this tokenizer successfully splits laugh/cry into 2 words. But the fallbacks are:\n",
    "- The link ‚Äòhttps://t.co/9z2J3P33Uc' is split into 7 words\n",
    "- world's is split into 2 words by \"'\" character\n",
    "- @datageneral is split into @ and datageneral\n",
    "- don't is split into do and n't\n",
    "\n",
    "Since these words should be considered as one word, this tokenizer is not what we want either. Is there a way that we can split words based on the space instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8575d84-d2ca-453d-ad59-6a931fe7813b",
   "metadata": {
    "tags": []
   },
   "source": [
    "**`TweetTokenizer`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "15d23102-990c-4261-8480-88878371c823",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tweet_tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3b441ada-92b8-4f97-9bd1-4f31ded4e0df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://t.co/9z2J3P33Uc']\n",
      "['laugh', '/', 'cry']\n",
      "['üò¨', 'üò≠', 'üòì', 'ü§¢', 'üôÑ', 'üò±']\n",
      "[\"world's\", 'problems']\n",
      "['@datageneral']\n",
      "[\"It's\", 'interesting']\n",
      "[\"don't\", 'spell', 'my', 'name', 'right']\n",
      "['all-nighter']\n",
      "['My', 'favorite', 'color', 'is', 'blue']\n",
      "['My', 'favorite', 'colors', 'are', 'blue', ',', 'red', ',', 'and', 'green', '.']\n"
     ]
    }
   ],
   "source": [
    "tweet_tokens = []\n",
    "\n",
    "for sent in compare_list:\n",
    "    \n",
    "    print(tweet_tokenizer.tokenize(sent))\n",
    "    \n",
    "    tweet_tokens.append(tweet_tokenizer.tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b398f6b8-9bb8-4672-9645-df675a4e2695",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
