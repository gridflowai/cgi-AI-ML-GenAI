{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e3fc039-6f08-48de-9622-fceb0a1f5b84",
   "metadata": {},
   "source": [
    "---------------------\n",
    "#### TextBlob \n",
    "\n",
    "- is a Python library that provides a simple API for common natural language processing (NLP) tasks, including tokenization.\n",
    "-----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "428585c0-4848-4dd2-9be6-7138ec1eae1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a30c5d8-67d7-4cc0-a040-cdc1c145f41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d03809c2-f056-429e-bc42-6212edfe734b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TextBlob', 'is', 'a', 'simple', 'library', 'for', 'processing', 'textual', 'data']\n"
     ]
    }
   ],
   "source": [
    "# Example text\n",
    "text = \"TextBlob is a simple library for processing textual data.\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "tokens = TextBlob(text).words\n",
    "\n",
    "# Print tokens\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86e2cab4-25b8-4a08-9c12-6ef5e2812672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'do', \"n't\", 'know', 'what', 'I', \"'d\", 'do', 'without', 'NLTK']\n"
     ]
    }
   ],
   "source": [
    "# Text containing contractions\n",
    "# not good\n",
    "text = \"I don't know what I'd do without NLTK.\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "tokens = TextBlob(text).words\n",
    "\n",
    "# Print tokens\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d093e34-ea5e-41bd-bf31-aaf9be5f8c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', \"'s\", 'going', 'to', 'the', 'park', 'tomorrow']\n"
     ]
    }
   ],
   "source": [
    "# Text containing contractions\n",
    "# not good\n",
    "text = \"He's going to the park tomorrow.\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "tokens = TextBlob(text).words\n",
    "\n",
    "# Print tokens\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c19254bd-0ca8-49e4-92f4-9dbe4bc97147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'ca', \"n't\", 'believe', 'it']\n"
     ]
    }
   ],
   "source": [
    "# Handling Contractions\n",
    "# not good\n",
    "text = \"I can't believe it!\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "tokens = TextBlob(text).words\n",
    "\n",
    "# Print tokens\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c114289-31c1-41f2-884b-4db2e741ba1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'m\", 'feeling', 'ðŸ˜ŠðŸ˜ŠðŸ˜ŠðŸ˜Š', 'today']\n"
     ]
    }
   ],
   "source": [
    "text = \"I'm feeling ðŸ˜ŠðŸ˜ŠðŸ˜ŠðŸ˜Š today!\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "tokens = TextBlob(text).words\n",
    "\n",
    "# Print tokens\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "46d02bbb-1503-43ac-824c-bbdd1db66459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'graduated', 'from', 'St', 'John', \"'s\", 'University']\n"
     ]
    }
   ],
   "source": [
    "# Abbreviations\n",
    "# not good\n",
    "text = \"He graduated from St. John's University.\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "tokens = TextBlob(text).words\n",
    "\n",
    "# Print tokens\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "296c4a12-bf70-48a4-9b3c-51a46a29d12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'high-speed', 'fence']\n"
     ]
    }
   ],
   "source": [
    "# Hyphenated Words\n",
    "# not good\n",
    "text = \"The quick brown fox jumped over the high-speed fence.\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "tokens = TextBlob(text).words\n",
    "\n",
    "# Print tokens\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac977f4-e7cf-4d0c-8168-37c4195eb8ba",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f6d7b97-46d7-468c-8b5b-a54afaf2ff88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compare_list = ['https://t.co/9z2J3P33Uc',\n",
    "               'laugh/cry',\n",
    "               'ðŸ˜¬ðŸ˜­ðŸ˜“ðŸ¤¢ðŸ™„ðŸ˜±',\n",
    "               \"world's problems\",\n",
    "               \"@datageneral\",\n",
    "                \"It's interesting\",\n",
    "               \"don't spell my name right\",\n",
    "               'all-nighter',\n",
    "                \"My favorite color is blue\",\n",
    "                \"My favorite colors are blue, red, and green.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c08f41b6-3106-4d9a-a06d-b2a340babf1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https', 't.co/9z2J3P33Uc']\n",
      "['laugh/cry']\n",
      "['ðŸ˜¬ðŸ˜­ðŸ˜“ðŸ¤¢ðŸ™„ðŸ˜±']\n",
      "['world', \"'s\", 'problems']\n",
      "['datageneral']\n",
      "['It', \"'s\", 'interesting']\n",
      "['do', \"n't\", 'spell', 'my', 'name', 'right']\n",
      "['all-nighter']\n",
      "['My', 'favorite', 'color', 'is', 'blue']\n",
      "['My', 'favorite', 'colors', 'are', 'blue', 'red', 'and', 'green']\n"
     ]
    }
   ],
   "source": [
    "for sent in compare_list:\n",
    "    \n",
    "    # Tokenize the text into words\n",
    "    tokens = TextBlob(sent).words\n",
    "\n",
    "    # Print tokens\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633add94-c959-4033-af14-52e276abd517",
   "metadata": {},
   "source": [
    "the results not GOOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec4e33c-cdc8-4165-ba72-3bf7b5bde823",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
