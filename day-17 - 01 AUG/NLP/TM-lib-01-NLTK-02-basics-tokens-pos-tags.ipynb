{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK - basics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is Tokenization?\n",
    "A token is a piece of a whole, so a word is a token in a sentence, and a sentence is a token in a paragraph. Tokenization is the process of splitting a string into a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My', 'favorite', 'color', 'is', 'blue']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystring = \"My favorite color is blue\"\n",
    "\n",
    "mystring.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mystring = \"My favorite colors are blue, red, and green.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My', 'favorite', 'colors', 'are', 'blue,', 'red,', 'and', 'green.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystring.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the punctuation marks are grouped in with their adjacent word (e.g. blue,). This is problematic for NLP applications, as the goal of tokenization is generally to divide a set (corpus) of documents into a common set of building blocks that can then be used as a basis for comparison. Hence, it’s no good if “blue” in \"My favorite color is blue\" doesn’t match with “blue” in \"My favorite colors are blue, red, and green.\" since the latter is tokenized as blue, rather than blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compare_list = ['https://t.co/9z2J3P33Uc',\n",
    "               'laugh/cry',\n",
    "               '😬😭😓🤢🙄😱',\n",
    "               \"world's problems\",\n",
    "               \"@datageneral\",\n",
    "                \"It's interesting\",\n",
    "               \"don't spell my name right\",\n",
    "               'all-nighter',\n",
    "                \"My favorite color is blue\",\n",
    "                \"My favorite colors are blue, red, and green.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. NLTK word_tokenize - separate words using spaces and punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['https', ':', '//t.co/9z2J3P33Uc'],\n",
       " ['laugh/cry'],\n",
       " ['😬😭😓🤢🙄😱'],\n",
       " ['world', \"'s\", 'problems'],\n",
       " ['@', 'datageneral'],\n",
       " ['It', \"'s\", 'interesting'],\n",
       " ['do', \"n't\", 'spell', 'my', 'name', 'right'],\n",
       " ['all-nighter'],\n",
       " ['My', 'favorite', 'color', 'is', 'blue'],\n",
       " ['My',\n",
       "  'favorite',\n",
       "  'colors',\n",
       "  'are',\n",
       "  'blue',\n",
       "  ',',\n",
       "  'red',\n",
       "  ',',\n",
       "  'and',\n",
       "  'green',\n",
       "  '.']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokens = []\n",
    "\n",
    "for sent in compare_list:\n",
    "\n",
    "    word_tokens.append(word_tokenize(sent))\n",
    "\n",
    "word_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dealing with well-formed, formal text, this standard word tokenizer makes a lot of sense and is likely to be sufficient. However, the same cannot be said for cases when our text data comes from more casual, slang-ridden sources like Twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@', 'john', 'lol', 'that', 'was', '#', 'awesome', ':', ')']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"@john lol that was #awesome :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "most likely that we’d prefer for\n",
    "\n",
    "- @ and john to be tokenized together as @john,\n",
    "- \\#  and awesome to be tokenized together as #awesome.\n",
    "This is because we’d expect that word usage in the context of hastags or at-mentions is likely different from usage in plain text.\n",
    "\n",
    "we would prefer that : and ) to be tokenized together as :), as :) is certainly more informative (e.g. for sentiment analysis) than the sum of its parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. WordPunctTokenizer\n",
    "WordPunctTokenizer splits all punctuations into separate tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compare_list = ['https://t.co/9z2J3P33Uc',\n",
    "               'laugh/cry',\n",
    "               '😬😭😓🤢🙄😱',\n",
    "               \"world's problems\",\n",
    "               \"@datageneral\",\n",
    "                \"It's interesting\",\n",
    "               \"don't spell my name right\",\n",
    "               'all-nighter',\n",
    "                \"My favorite color is blue\",\n",
    "                \"My favorite colors are blue, red, and green.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['https', '://', 't', '.', 'co', '/', '9z2J3P33Uc'],\n",
       " ['laugh', '/', 'cry'],\n",
       " ['😬😭😓🤢🙄😱'],\n",
       " ['world', \"'\", 's', 'problems'],\n",
       " ['@', 'datageneral'],\n",
       " ['It', \"'\", 's', 'interesting'],\n",
       " ['don', \"'\", 't', 'spell', 'my', 'name', 'right'],\n",
       " ['all', '-', 'nighter'],\n",
       " ['My', 'favorite', 'color', 'is', 'blue'],\n",
       " ['My',\n",
       "  'favorite',\n",
       "  'colors',\n",
       "  'are',\n",
       "  'blue',\n",
       "  ',',\n",
       "  'red',\n",
       "  ',',\n",
       "  'and',\n",
       "  'green',\n",
       "  '.']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punct_tokenizer = WordPunctTokenizer()\n",
    "\n",
    "punct_tokens = []\n",
    "\n",
    "for sent in compare_list:\n",
    "    \n",
    "    punct_tokens.append(punct_tokenizer.tokenize(sent))\n",
    "punct_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this tokenizer successfully splits laugh/cry into 2 words. But the fallbacks are:\n",
    "- The link ‘https://t.co/9z2J3P33Uc' is split into 7 words\n",
    "- world's is split into 2 words by \"'\" character\n",
    "- @datageneral is split into @ and datageneral\n",
    "- don't is split into do and n't\n",
    "\n",
    "Since these words should be considered as one word, this tokenizer is not what we want either. Is there a way that we can split words based on the space instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3. TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tweet_tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://t.co/9z2J3P33Uc']\n",
      "['laugh', '/', 'cry']\n",
      "['😬', '😭', '😓', '🤢', '🙄', '😱']\n",
      "[\"world's\", 'problems']\n",
      "['@datageneral']\n",
      "[\"It's\", 'interesting']\n",
      "[\"don't\", 'spell', 'my', 'name', 'right']\n",
      "['all-nighter']\n",
      "['My', 'favorite', 'color', 'is', 'blue']\n",
      "['My', 'favorite', 'colors', 'are', 'blue', ',', 'red', ',', 'and', 'green', '.']\n"
     ]
    }
   ],
   "source": [
    "tweet_tokens = []\n",
    "\n",
    "for sent in compare_list:\n",
    "    \n",
    "    print(tweet_tokenizer.tokenize(sent))\n",
    "    \n",
    "    tweet_tokens.append(tweet_tokenizer.tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EXAMPLE_TEXT = \"Hello Mr. Smith, how are you doing today? \\\n",
    "               The weather is great, and Python is awesome !\\\n",
    "               The sky is pinkish-blue. \\\n",
    "               You shouldn\\'t eat cardboard.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentence = \"\"\"At eight o'clock on Thursday morning Arthur felt very good. But he didn't go to play\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentence tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello Mr. Smith, how are you doing today?',\n",
       " 'The weather is great, and Python is awesome !',\n",
       " 'The sky is pinkish-blue.',\n",
       " \"You shouldn't eat cardboard.\"]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(EXAMPLE_TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Mr. Smith, how are you doing today?\n",
      "The weather is great, and Python is awesome !\n",
      "The sky is pinkish-blue.\n",
      "You shouldn't eat cardboard.\n"
     ]
    }
   ],
   "source": [
    "for sent in sent_tokenize(EXAMPLE_TEXT):\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there, we have created tokens, which are sentences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['At',\n",
       " 'eight',\n",
       " \"o'clock\",\n",
       " 'on',\n",
       " 'Thursday',\n",
       " 'morning',\n",
       " 'Arthur',\n",
       " 'felt',\n",
       " 'very',\n",
       " 'good',\n",
       " '.',\n",
       " 'But',\n",
       " 'he',\n",
       " 'did',\n",
       " \"n't\",\n",
       " 'go',\n",
       " 'to',\n",
       " 'play']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Mr.',\n",
       " 'Smith',\n",
       " ',',\n",
       " 'how',\n",
       " 'are',\n",
       " 'you',\n",
       " 'doing',\n",
       " 'today',\n",
       " '?',\n",
       " 'The',\n",
       " 'weather',\n",
       " 'is',\n",
       " 'great',\n",
       " ',',\n",
       " 'and',\n",
       " 'Python',\n",
       " 'is',\n",
       " 'awesome',\n",
       " '!',\n",
       " 'The',\n",
       " 'sky',\n",
       " 'is',\n",
       " 'pinkish-blue',\n",
       " '.',\n",
       " 'You',\n",
       " 'should',\n",
       " \"n't\",\n",
       " 'eat',\n",
       " 'cardboard',\n",
       " '.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(EXAMPLE_TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation. \n",
    "- First, notice that punctuation is treated as a separate token. \n",
    "- Also, notice the separation of the word \"shouldn't\" into \"should\" and \"n't.\" \n",
    "- Finally, notice that \"pinkish-blue\" is indeed treated like the \"one word\" it was meant to be turned into\n",
    "\n",
    "- Some words seem trivial - these are a form of \"stop words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"this is Ram's text, is'nt it?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', \"Ram's\", 'text,', \"is'nt\", 'it?']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'Ram', \"'s\", 'text', ',', \"is'nt\", 'it', '?']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'Ram', \"'\", 's', 'text', ',', 'is', \"'\", 'nt', 'it', '?']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'off', 'again', 'ma', 'that', 'into', 'its', 'having', 'myself', 'with', 'our', 'now', 'such', 'an', 'll', 'wasn', 'where', 'before', 'you', 'she', 'of', 'your', 'mightn', 'too', \"haven't\", 'don', 'him', 'her', 'y', 'o', 'up', \"hasn't\", 'in', 'am', 'haven', 'below', \"you'd\", \"you're\", 'above', 'from', \"it's\", 'my', 'which', 'being', 'own', 'm', 'we', 'do', 'by', 'more', 'it', 'shouldn', 'yours', \"shouldn't\", 'so', 'wouldn', 'are', \"she's\", 'whom', 'doing', 'over', \"won't\", 'very', \"mightn't\", 'or', 'couldn', 'is', 'few', 'what', 'because', \"couldn't\", 'ourselves', 'will', 'themselves', 'against', 'while', 'them', 'no', \"should've\", 'on', 'hers', \"weren't\", 'all', 'herself', 'they', 'was', 'won', 'has', 'i', 'should', \"wasn't\", 'does', 'needn', 'some', 'these', 'there', 'once', 'just', 'as', 'were', 'isn', 'until', 're', 'at', 'and', \"needn't\", 'not', 's', 'yourselves', 'other', 'for', 'have', \"you've\", 'his', 'had', 'nor', 'any', \"don't\", 'most', 'aren', \"aren't\", 'each', 'yourself', 'those', 'weren', \"doesn't\", 'himself', 'theirs', \"isn't\", 'their', 'but', 'ain', \"hadn't\", \"mustn't\", \"shan't\", \"that'll\", 'hasn', 'can', 'didn', 'itself', 'd', 'shan', 'under', 'to', 'down', 'here', 'why', 'me', 'be', 'mustn', 'both', 'been', 'who', 'through', 'between', 'when', 't', 'during', 'after', 'out', 'a', 'how', 'than', \"didn't\", 'doesn', 'hadn', 'then', 'only', 'if', 'he', 'further', 'same', 've', 'about', \"you'll\", 'ours', 'this', \"wouldn't\", 'did', 'the'}\n"
     ]
    }
   ],
   "source": [
    "print(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sent = \"This is a sample sentence, showing off the stop words filtration.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens = word_tokenize(example_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
      "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "# option 1\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "# option 2\n",
    "filtered_sentence = []\n",
    "\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "print(word_tokens)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming words\n",
    "\n",
    "The idea of stemming is a sort of normalizing method. Many variations of words carry the same meaning, other than when tense is involved.\n",
    "\n",
    "The reason why we stem is to shorten the lookup, and normalize sentences.\n",
    "\n",
    "Consider:\n",
    "\n",
    "I was taking a ride in the car.\n",
    "I was riding in the car.\n",
    "\n",
    "One of the most popular stemming algorithms is the __Porter stemmer__, which has been around since 1979."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "porter   = PorterStemmer()\n",
    "lancaster= LancasterStemmer()\n",
    "sno      = nltk.stem.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                 Porter Stemmer       lancaster Stemmer    Snowball Stemmer    \n",
      "cave                 cave                 cav                  cave                \n",
      "caver                caver                cav                  caver               \n",
      "caved                cave                 cav                  cave                \n"
     ]
    }
   ],
   "source": [
    "word_list = [\"cave\", \"caver\", \"caved\"]\n",
    "\n",
    "print(\"{0:20} {1:20} {2:20} {3:20}\".format(\"Word\",\"Porter Stemmer\", \"lancaster Stemmer\", \"Snowball Stemmer\"))\n",
    "\n",
    "for word in word_list:\n",
    "    print(\"{0:20} {1:20} {2:20} {3:20}\".format(word, porter.stem(word), lancaster.stem(word), sno.stem(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                 Porter Stemmer       lancaster Stemmer    Snowball Stemmer    \n",
      "run                  run                  run                  run                 \n",
      "ran                  ran                  ran                  ran                 \n",
      "runner               runner               run                  runner              \n",
      "running              run                  run                  run                 \n"
     ]
    }
   ],
   "source": [
    "word_list = [\"run\", \"ran\", \"runner\", \"running\"]\n",
    "\n",
    "print(\"{0:20} {1:20} {2:20} {3:20}\".format(\"Word\",\"Porter Stemmer\", \"lancaster Stemmer\", \"Snowball Stemmer\"))\n",
    "\n",
    "for word in word_list:\n",
    "    print(\"{0:20} {1:20} {2:20} {3:20}\".format(word, porter.stem(word), lancaster.stem(word), sno.stem(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                 Porter Stemmer       lancaster Stemmer    Snowball Stemmer    \n",
      "cats                 cat                  cat                  cat                 \n",
      "trouble              troubl               troubl               troubl              \n",
      "troubling            troubl               troubl               troubl              \n",
      "troubled             troubl               troubl               troubl              \n",
      "troublesome          troublesom           troublesom           troublesom          \n"
     ]
    }
   ],
   "source": [
    "word_list = [\"cats\", \"trouble\", \"troubling\", \"troubled\", \"troublesome\"]\n",
    "\n",
    "print(\"{0:20} {1:20} {2:20} {3:20}\".format(\"Word\",\"Porter Stemmer\", \"lancaster Stemmer\", \"Snowball Stemmer\"))\n",
    "\n",
    "for word in word_list:\n",
    "    print(\"{0:20} {1:20} {2:20} {3:20}\".format(word, porter.stem(word), lancaster.stem(word), sno.stem(word))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the PorterStemmer is \n",
    "- giving the root (stem) of the word \"cats\" by simply removing the 's' after cat. This is a suffix added to cat to make it plural. \n",
    "- But if we look at 'trouble', 'troubling' and 'troubled' they are stemmed to 'trouble' because **PorterStemmer algorithm does not follow linguistics rather a set of 05 rules for different cases that are applied in phases (step by step) to generate stems**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                 Porter Stemmer       lancaster Stemmer    Snowball Stemmer    \n",
      "argue                argu                 argu                 argu                \n",
      "argued               argu                 argu                 argu                \n",
      "argues               argu                 argu                 argu                \n",
      "arguing              argu                 argu                 argu                \n",
      "argus                argu                 arg                  argus               \n"
     ]
    }
   ],
   "source": [
    "word_list = [\"argue\", \"argued\", \"argues\", \"arguing\", \"argus\"]\n",
    "\n",
    "print(\"{0:20} {1:20} {2:20} {3:20}\".format(\"Word\",\"Porter Stemmer\", \"lancaster Stemmer\", \"Snowball Stemmer\"))\n",
    "\n",
    "for word in word_list:\n",
    "    print(\"{0:20} {1:20} {2:20} {3:20}\".format(word, porter.stem(word), lancaster.stem(word), sno.stem(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                 Porter Stemmer       lancaster Stemmer    Snowball Stemmer    \n",
      "friend               friend               friend               friend              \n",
      "friendship           friendship           friend               friendship          \n",
      "friends              friend               friend               friend              \n",
      "friendships          friendship           friend               friendship          \n",
      "stabil               stabil               stabl                stabil              \n",
      "destabilize          destabil             dest                 destabil            \n",
      "misunderstanding     misunderstand        misunderstand        misunderstand       \n",
      "railroad             railroad             railroad             railroad            \n",
      "moonlight            moonlight            moonlight            moonlight           \n",
      "football             footbal              footbal              footbal             \n"
     ]
    }
   ],
   "source": [
    "#A list of words to be stemmed\n",
    "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\",\"stabil\",\"destabilize\",\"misunderstanding\",\"railroad\",\"moonlight\",\"football\"]\n",
    "\n",
    "print(\"{0:20} {1:20} {2:20} {3:20}\".format(\"Word\",\"Porter Stemmer\", \"lancaster Stemmer\", \"Snowball Stemmer\"))\n",
    "for word in word_list:\n",
    "    print(\"{0:20} {1:20} {2:20} {3:20}\".format(word, porter.stem(word), lancaster.stem(word), sno.stem(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pythoners are very intelligent and work very pythonly and now they are pythoning their way to success.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence=\"Pythoners are very intelligent and work very pythonly and now they are pythoning their way to success.\"\n",
    "porter.stem(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stemmer sees the entire sentence as a word, so it returns it as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My system keep crash hi crash yesterday, our crash daili\n",
      "my system keep crash his crash yesterday, our crash dai\n"
     ]
    }
   ],
   "source": [
    "text = \"My system keeps crashing his crashed yesterday, ours crashes daily\"\n",
    "\n",
    "print(' '.join([porter.stem(word) for word in text.split()]))\n",
    "print(' '.join([lancaster.stem(word) for word in text.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lemmatization\n",
    "\n",
    "Lemmatization is the process of converting a word to its base form. \n",
    "\n",
    "The difference between stemming and lemmatization is, \n",
    "\n",
    "> lemmatization considers the context and converts the word to its meaningful base form, whereas stemming just removes the last few characters, often leading to incorrect meanings and spelling errors.\n",
    "\n",
    "For example, lemmatization would correctly identify the base form of ‘caring’ to ‘care’, whereas, stemming would cutoff the ‘ing’ part and convert it to car.\n",
    "\n",
    "    ‘Caring’ -> Lemmatization -> ‘Care’\n",
    "    ‘Caring’ -> Stemming -> ‘Car’\n",
    "    \n",
    "ways to lemmatize:-\n",
    "\n",
    "    Wordnet Lemmatizer\n",
    "    Spacy Lemmatizer\n",
    "    TextBlob\n",
    "    CLiPS Pattern\n",
    "    Stanford CoreNLP\n",
    "    Gensim Lemmatizer\n",
    "    TreeTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Init the Wordnet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                 WordNetLemmatizer   \n",
      "friend               friend               \n",
      "friendship           friendship           \n",
      "friends              friend               \n",
      "friendships          friendship           \n",
      "stabilize            stabilize            \n",
      "destabilize          destabilize          \n",
      "misunderstanding     misunderstanding     \n",
      "railroad             railroad             \n",
      "moonlight            moonlight            \n",
      "football             football             \n"
     ]
    }
   ],
   "source": [
    "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\",\"stabilize\",\"destabilize\",\"misunderstanding\",\"railroad\",\"moonlight\",\"football\"]\n",
    "\n",
    "print(\"{0:20} {1:20}\".format(\"Word\",\"WordNetLemmatizer\"))\n",
    "\n",
    "for word in word_list:\n",
    "    print(\"{0:20} {1:20} \".format(word, lemmatizer.lemmatize(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bat\n",
      "are\n",
      "foot\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize Single Word\n",
    "print(lemmatizer.lemmatize(\"bats\"))\n",
    "\n",
    "print(lemmatizer.lemmatize(\"are\"))\n",
    "\n",
    "print(lemmatizer.lemmatize(\"feet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'striped', 'bats', 'are', 'hanging', 'on', 'their', 'feet', 'for', 'best']\n"
     ]
    }
   ],
   "source": [
    "# Define the sentence to be lemmatized\n",
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "\n",
    "# Tokenize: Split the sentence into words\n",
    "word_list = nltk.word_tokenize(sentence)\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The --> The\n",
      "striped --> striped\n",
      "bats --> bat\n",
      "are --> are\n",
      "hanging --> hanging\n",
      "on --> on\n",
      "their --> their\n",
      "feet --> foot\n",
      "for --> for\n",
      "best --> best\n"
     ]
    }
   ],
   "source": [
    "for w in word_list:\n",
    "    print(w, '-->', lemmatizer.lemmatize(w) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice it didn’t do a good job. Because, ‘are’ is not converted to ‘be’ and ‘hanging’ is not converted to ‘hang’ as expected. \n",
    "\n",
    "This can be corrected if we provide the correct ‘part-of-speech’ tag (POS tag) as the second argument to lemmatize()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strip\n",
      "stripe\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"stripes\", 'v')) \n",
    "print(lemmatizer.lemmatize(\"stripes\", 'n'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the N-grams for the given sentence\n",
    "\n",
    "The essential concepts in text mining is n-grams, which are a set of co-occurring or continuous sequence of n items from a sequence of large text or sentence. The item here could be words, letters, and syllables. 1-gram is also called as unigrams are the unique words present in the sentence. Bigram(2-gram) is the combination of 2 words. Trigram(3-gram) is 3 words and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = 'Data science is a wonderful program, \\\n",
    "Data science is a land of opportunities,data science is about machine learning '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grams = 3\n",
    "n_grams = ngrams(nltk.word_tokenize(text), grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<zip at 0x15d5da36000>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Data', 'science', 'is'),\n",
       " ('science', 'is', 'a'),\n",
       " ('is', 'a', 'wonderful'),\n",
       " ('a', 'wonderful', 'program'),\n",
       " ('wonderful', 'program', ','),\n",
       " ('program', ',', 'Data'),\n",
       " (',', 'Data', 'science'),\n",
       " ('Data', 'science', 'is'),\n",
       " ('science', 'is', 'a'),\n",
       " ('is', 'a', 'land'),\n",
       " ('a', 'land', 'of'),\n",
       " ('land', 'of', 'opportunities'),\n",
       " ('of', 'opportunities', ','),\n",
       " ('opportunities', ',', 'data'),\n",
       " (',', 'data', 'science'),\n",
       " ('data', 'science', 'is'),\n",
       " ('science', 'is', 'about'),\n",
       " ('is', 'about', 'machine'),\n",
       " ('about', 'machine', 'learning')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nltk.trigrams(word_tokenize(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data science is',\n",
       " 'science is a',\n",
       " 'is a wonderful',\n",
       " 'a wonderful program',\n",
       " 'wonderful program ,',\n",
       " 'program , Data',\n",
       " ', Data science',\n",
       " 'Data science is',\n",
       " 'science is a',\n",
       " 'is a land',\n",
       " 'a land of',\n",
       " 'land of opportunities',\n",
       " 'of opportunities ,',\n",
       " 'opportunities , data',\n",
       " ', data science',\n",
       " 'data science is',\n",
       " 'science is about',\n",
       " 'is about machine',\n",
       " 'about machine learning']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[' '.join(grams) for grams in n_grams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Data science is a wonderful program, \\\n",
    "Data science is a land of opportunities,data science is about machine learning '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = list(nltk.bigrams(word_tokenize(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('Data', 'science'): 2,\n",
       "         ('science', 'is'): 3,\n",
       "         ('is', 'a'): 2,\n",
       "         ('a', 'wonderful'): 1,\n",
       "         ('wonderful', 'program'): 1,\n",
       "         ('program', ','): 1,\n",
       "         (',', 'Data'): 1,\n",
       "         ('a', 'land'): 1,\n",
       "         ('land', 'of'): 1,\n",
       "         ('of', 'opportunities'): 1,\n",
       "         ('opportunities', ','): 1,\n",
       "         (',', 'data'): 1,\n",
       "         ('data', 'science'): 1,\n",
       "         ('is', 'about'): 1,\n",
       "         ('about', 'machine'): 1,\n",
       "         ('machine', 'learning'): 1})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OR equivalenty ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram:  ['A', 'class', 'is', 'a', 'blueprint', 'for', 'the', 'object', '.']\n",
      "2-gram:  ['A class', 'class is', 'is a', 'a blueprint', 'blueprint for', 'for the', 'the object', 'object .']\n",
      "3-gram:  ['A class is', 'class is a', 'is a blueprint', 'a blueprint for', 'blueprint for the', 'for the object', 'the object .']\n",
      "4-gram:  ['A class is a', 'class is a blueprint', 'is a blueprint for', 'a blueprint for the', 'blueprint for the object', 'for the object .']\n"
     ]
    }
   ],
   "source": [
    "# Function to generate n-grams from sentences.\n",
    "def extract_ngrams(data, num):\n",
    "    n_grams = ngrams(nltk.word_tokenize(data), num)\n",
    "    return [ ' '.join(grams) for grams in n_grams]\n",
    " \n",
    "data = 'A class is a blueprint for the object.'\n",
    " \n",
    "print(\"1-gram: \", extract_ngrams(data, 1))\n",
    "print(\"2-gram: \", extract_ngrams(data, 2))\n",
    "print(\"3-gram: \", extract_ngrams(data, 3))\n",
    "print(\"4-gram: \", extract_ngrams(data, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Data', 'science', 'is'),\n",
       " ('science', 'is', 'a'),\n",
       " ('is', 'a', 'wonderful'),\n",
       " ('a', 'wonderful', 'program'),\n",
       " ('wonderful', 'program', ','),\n",
       " ('program', ',', 'Data'),\n",
       " (',', 'Data', 'science'),\n",
       " ('Data', 'science', 'is'),\n",
       " ('science', 'is', 'a'),\n",
       " ('is', 'a', 'land'),\n",
       " ('a', 'land', 'of'),\n",
       " ('land', 'of', 'opportunities'),\n",
       " ('of', 'opportunities', ','),\n",
       " ('opportunities', ',', 'data'),\n",
       " (',', 'data', 'science'),\n",
       " ('data', 'science', 'is'),\n",
       " ('science', 'is', 'about'),\n",
       " ('is', 'about', 'machine'),\n",
       " ('about', 'machine', 'learning')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nltk.trigrams(word_tokenize(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parts of speech tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Dummy text \n",
    "txt = '''Sukanya, Bhupen and Bhavik are my good friends. Sukanya is getting married next year. Marriage is a big step in one’s life.\\  \n",
    "       It is both exciting and frightening. But friendship is a sacred bond between people. \\  \n",
    "       It is a special kind of love between us. \\\n",
    "       Many of you must have tried searching for a friend \\\n",
    "       but never found the right one.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized = sent_tokenize(txt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Sukanya', 'NNP'), (',', ','), ('Bhupen', 'NNP'), ('and', 'CC'), ('Bhavik', 'NNP'), ('are', 'VBP'), ('my', 'PRP$'), ('good', 'JJ'), ('friends', 'NNS'), ('.', '.')]\n",
      "[('Sukanya', 'NNP'), ('is', 'VBZ'), ('getting', 'VBG'), ('married', 'VBN'), ('next', 'JJ'), ('year', 'NN'), ('.', '.')]\n",
      "[('Marriage', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('big', 'JJ'), ('step', 'NN'), ('in', 'IN'), ('one', 'CD'), ('’', 'NN'), ('s', 'NN'), ('life.\\\\', 'NN'), ('It', 'PRP'), ('is', 'VBZ'), ('both', 'DT'), ('exciting', 'VBG'), ('and', 'CC'), ('frightening', 'NN'), ('.', '.')]\n",
      "[('But', 'CC'), ('friendship', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('sacred', 'JJ'), ('bond', 'NN'), ('between', 'IN'), ('people', 'NNS'), ('.', '.')]\n",
      "[('\\\\', 'VB'), ('It', 'PRP'), ('is', 'VBZ'), ('a', 'DT'), ('special', 'JJ'), ('kind', 'NN'), ('of', 'IN'), ('love', 'NN'), ('between', 'IN'), ('us', 'PRP'), ('.', '.')]\n",
      "[('Many', 'JJ'), ('of', 'IN'), ('you', 'PRP'), ('must', 'MD'), ('have', 'VB'), ('tried', 'VBN'), ('searching', 'VBG'), ('for', 'IN'), ('a', 'DT'), ('friend', 'NN'), ('but', 'CC'), ('never', 'RB'), ('found', 'VBD'), ('the', 'DT'), ('right', 'JJ'), ('one', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "for i in tokenized: \n",
    "      \n",
    "    # Word tokenizers is used to find the words  \n",
    "    # and punctuation in a string \n",
    "    wordsList = nltk.word_tokenize(i) \n",
    "  \n",
    "    # removing stop words from wordList \n",
    "    #wordsList = [w for w in wordsList if not w in stop_words]  \n",
    "  \n",
    "    #  Using a Tagger. Which is part-of-speech  \n",
    "    # tagger or POS-tagger.  \n",
    "    tagged = nltk.pos_tag(wordsList) \n",
    "  \n",
    "    print(tagged) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one more example .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Today', 'NN'),\n",
       " ('I', 'PRP'),\n",
       " ('will', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('learning', 'VBG'),\n",
       " ('about', 'IN'),\n",
       " ('POS', 'NNP'),\n",
       " ('tags', 'NNS')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import tag\n",
    "\n",
    "sent = 'Today I will be learning about POS tags'\n",
    "\n",
    "tagged_sent = tag.pos_tag(word_tokenize(sent))\n",
    "\n",
    "tagged_sent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------\n",
    "#### Ready-made NE (Named Entity) chunker\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|NE Type |Examples |\n",
    "|--------|---------|\n",
    "|ORGANIZATION | Georgia-Pacific Corp., WHO |\n",
    "|PERSON | Eddy Bonte, President Obama|\n",
    "|LOCATION | Murray River, Mount Everest|\n",
    "|DATE | June, 2008-06-29 |\n",
    "|TIME | two fifty a m, 1:30 p.m.|\n",
    "|MONEY | 175 million Canadian Dollars, GBP 10.40|\n",
    "|PERCENT | twenty pct, 18.75 %|\n",
    "|FACILITY | Washington Monument, Stonehenge\n",
    "|GPE | South East Asia, Midlothian |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Infosys/NNP)\n",
      "  is/VBZ\n",
      "  IT/NNP\n",
      "  service/NN\n",
      "  provider/NN\n",
      "  ,/,\n",
      "  based/VBN\n",
      "  out/IN\n",
      "  of/IN\n",
      "  banaglore/NN\n",
      "  ./.\n",
      "  Was/NNP\n",
      "  established/VBN\n",
      "  on/IN\n",
      "  1st/CD\n",
      "  Jan/NNP\n",
      "  1990it/CD\n",
      "  is/VBZ\n",
      "  making/VBG\n",
      "  a/DT\n",
      "  profit/NN\n",
      "  of/IN\n",
      "  56/CD\n",
      "  %/NN\n",
      "  ./.\n",
      "  (PERSON Annual/JJ)\n",
      "  turnover/NN\n",
      "  is/VBZ\n",
      "  500/CD\n",
      "  million/CD\n",
      "  US/NNP\n",
      "  $/$\n",
      "  made/VBD\n",
      "  a/DT\n",
      "  profit/NN\n",
      "  of/IN\n",
      "  77/CD\n",
      "  %/NN)\n"
     ]
    }
   ],
   "source": [
    "from nltk import chunk, tag\n",
    "\n",
    "sent = 'Infosys is IT service provider, based out of banaglore. Was established on 1st Jan 1990\\\n",
    "it is making a profit of 56%. Annual turnover is 500 million US$ made a profit of 77%'\n",
    "\n",
    "tagged_sent = tag.pos_tag(word_tokenize(sent))\n",
    "\n",
    "tree = chunk.ne_chunk(tagged_sent)\n",
    "\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ne_subtrees1 = tree.subtrees(filter=lambda t: t.label() in\n",
    "                                                            [\"ORGANIZATION\", \"PERSON\",\n",
    "                                                            \"LOCATION\",      \"DATE\",\"TIME\",\n",
    "                                                            \"MONEY\",         \"PERCENT\",\n",
    "                                                            \"FACILITY\",      \"GPE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Tree.subtrees at 0x0000015D5D9E05E0>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ne_subtrees1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne_subtrees2 = tree.subtrees(filter=lambda t: t.label() in [\"NNP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tree('GPE', [('Infosys', 'NNP')]), Tree('PERSON', [('Annual', 'JJ')])]\n"
     ]
    }
   ],
   "source": [
    "ne_subtrees_list1 = [tree for tree in ne_subtrees1]\n",
    "print(ne_subtrees_list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "ne_subtrees_list2 = [tree for tree in ne_subtrees2]\n",
    "print(ne_subtrees_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
