{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8c8ba65-40c8-4b87-8364-d1e2af13c643",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "#### Tokenization with Spacy\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6224fa1-49a1-4d82-a197-03097551059e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52bfe69e-a6e7-45e5-ba60-899134c66b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "84507aa1-e469-47e6-a2ff-be329e412b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9f8b40-65d7-4bec-a53b-9d60997cd390",
   "metadata": {},
   "source": [
    "**Hyphenated Words**\n",
    "    \n",
    "NOT GOOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d1fa549-d580-4d7e-a082-3cac8a04d795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Spacy', 'is', 'a', 'state', '-', 'of', '-', 'the', '-', 'art', 'natural', 'language', 'processing', 'library', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Spacy is a state-of-the-art natural language processing library.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f28bbda-e8b2-4ee3-9e3d-cb28af1f3294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['severe', 'run', '-', 'down']\n"
     ]
    }
   ],
   "source": [
    "text = \"severe run-down\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ddca9225-c188-4b67-886e-426ff9188636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Self', '-', 'driving', 'cars', 'are', 'becoming', 'more', 'common', 'in', 'modern', 'society', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Self-driving cars are becoming more common in modern society.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ed518c-720c-49d6-861b-f3984ab92701",
   "metadata": {},
   "source": [
    "**Comma Separated Words:**\n",
    "\n",
    "Spacy's tokenizer splits words separated by commas into separate tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5e339384-2243-4ec6-b7c5-c38a3df6edcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'like', 'apples', ',', 'oranges', ',', 'and', 'bananas', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"I like apples, oranges, and bananas.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a9d0b4-c6e2-4e36-88e0-02af6d34e1d0",
   "metadata": {},
   "source": [
    "**Abbreviations:**\n",
    "\n",
    "Spacy's tokenizer treats abbreviations as single tokens unless explicitly split by punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5b5d809c-693f-41d4-b40f-99b9781a66ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dr.', 'Smith', 'is', 'a', 'Ph.D.', 'graduate', 'from', 'MIT', '.', 'St.', 'Xvaiers']\n"
     ]
    }
   ],
   "source": [
    "text = \"Dr. Smith is a Ph.D. graduate from MIT. St. Xvaiers\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4009534f-636d-486a-b790-3b5905be5c25",
   "metadata": {},
   "source": [
    "**Contractions**\n",
    "\n",
    "Spacy's tokenizer splits contractions into separate tokens.\n",
    "\n",
    "NOT GOOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a202638c-fb61-481a-bd92-d12a5c37f267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'ca', \"n't\", 'believe', 'it', \"'s\", 'already', 'evening', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"I can't believe it's already evening.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df28edc7-d2e8-4f1a-a556-84962adf3757",
   "metadata": {},
   "source": [
    "**Punctuations**\n",
    "\n",
    "Spacy's tokenizer treats punctuations as separate tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c53624f9-7662-4308-8ec0-9d1747e518cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'cat', 'jumped', '@over', 'the', 'fence', '!']\n"
     ]
    }
   ],
   "source": [
    "text = \"The cat jumped @over the fence!\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd21e80-f537-4968-acfa-4411d5ce6d77",
   "metadata": {},
   "source": [
    "#### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1f40b801-352d-4c6a-a740-3d777995543f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apple', 'is', 'looking', 'at', 'buying', 'U.K.', 'startup', 'for', '$', '1', 'billion']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "    \n",
    "# Tokenize the text\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2197dcf-735e-4a53-b6b3-92e401e290e1",
   "metadata": {},
   "source": [
    "**Pros of Spacy Tokenizers:**\n",
    "- Efficient and fast tokenization, especially for large datasets.\n",
    "- Handles various linguistic structures and edge cases effectively.\n",
    "- Provides support for multiple languages and pre-trained models.\n",
    "- Integrated with other NLP functionalities like part-of-speech tagging, named entity recognition, and dependency parsing.\n",
    "\n",
    "**Cons of Spacy Tokenizers:**\n",
    "- Tokenization rules may not be customizable compared to other libraries like NLTK.\n",
    "- Less suitable for specialized tokenization tasks requiring highly customized rules.\n",
    "- Requires additional language models to be downloaded for non-English text processing.\n",
    "- Heavier memory footprint compared to simpler tokenization libraries for basic tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be7ea1e-886b-4995-a0ee-af86e5a71592",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
