{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d35e5c1a-17e5-4ad7-ab39-4a4b814727e4",
   "metadata": {},
   "source": [
    "------------------\n",
    "#### Gensim, \n",
    "- tokenization is often used in the context of `topic modeling`, `document similarity analysis`, and other natural language processing tasks. \n",
    "- Gensim provides various functions and classes for tokenization, including `simple pre-processing steps` and more advanced tokenization techniques. \n",
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a81467e-3b27-4c84-85b5-7d636462d878",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbe644af-0048-4a9c-8d60-06c74d98bd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text\n",
    "text = \"Gensim provides robust tokenization functionality for NLP tasks.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c68b47b-b68b-4e87-af82-fcb0c9632be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokens = simple_preprocess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dfaf0b56-b862-4188-aac1-e3c106a57808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gensim', 'provides', 'robust', 'tokenization', 'functionality', 'for', 'nlp', 'tasks']\n"
     ]
    }
   ],
   "source": [
    "# Print tokens\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac522861-af78-4095-9a9d-c2a372e5fb3e",
   "metadata": {},
   "source": [
    "### Parameters:\n",
    "\n",
    "- **doc**: str or iterable of str\n",
    "  - The input document or iterable of documents to be tokenized.\n",
    "  \n",
    "- **deacc**: bool, optional (default=True)\n",
    "  - Whether to remove accents from tokens. Setting this to True will remove accents from tokens using `unidecode`.\n",
    "  \n",
    "- **min_len**: int, optional (default=2)\n",
    "  - Minimum length of tokens to be included in the output. Tokens shorter than this length will be excluded from the output.\n",
    "  \n",
    "- **max_len**: int, optional (default=15)\n",
    "  - Maximum length of tokens to be included in the output. Tokens longer than this length will be excluded from the output.\n",
    "  \n",
    "- **lowercase**: bool, optional (default=True)\n",
    "  - Whether to convert the text to lowercase before tokenization.\n",
    "  \n",
    "- **no_above**: float, optional (default=1.0)\n",
    "  - Threshold for filtering out tokens based on document frequency. Tokens with a document frequency higher than this threshold will be excluded from the output.\n",
    "  \n",
    "- **no_below**: int, optional (default=5)\n",
    "  - Threshold for filtering out tokens based on document frequency. Tokens with a document frequency lower than this threshold will be excluded from the output.\n",
    "\n",
    "- **keep_n**: int, optional (default=100000)\n",
    "  - Maximum number of tokens to keep in the vocabulary. If set, the vocabulary will be trimmed to keep only the top `keep_n` tokens based on document frequency.\n",
    "\n",
    "- **tokenizer**: function, optional (default=None)\n",
    "  - Custom tokenizer function to be used for tokenization. If provided, this function will be used instead of the default tokenizer.\n",
    "\n",
    "- **token_filters**: iterable of functions, optional (default=None)\n",
    "  - Iterable of custom token filter functions to be applied after tokenization. Each function in the iterable should take a token as input and return either True or False to indicate whether the token should be included in the output.\n",
    "\n",
    "- **max_doc_len**: int, optional (default=float('inf'))\n",
    "  - Maximum length of documents to be processed. Documents longer than this length will be truncated.\n",
    "\n",
    "### Returns:\n",
    "- List of str\n",
    "  - A list of tokens extracted from the input document(s).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b229973-2582-40be-a22b-e729d68592c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he', 'going', 'to', 'the', 'park', 'tomorrow']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text containing contractions\n",
    "# OK\n",
    "text = \"He's going to the park tomorrow.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = simple_preprocess(text)\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "efe30601-31ba-4504-a56a-d1b51ac2807d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['can', 'believe', 'it']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text containing contractions\n",
    "# not good\n",
    "text = \"I can't believe it!\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = simple_preprocess(text)\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f7d1024-379c-43b1-a591-10d73803371d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feeling', 'today']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text containing contractions\n",
    "# not good\n",
    "text = \"I'm feeling ðŸ˜ŠðŸ˜ŠðŸ˜ŠðŸ˜Š today!\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = simple_preprocess(text)\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc3298f6-3353-4bbd-b130-aebe41fe3b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he', 'graduated', 'from', 'st', 'john', 'university']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Abbreviations\n",
    "# not good\n",
    "text = \"He graduated from St. John's University.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = simple_preprocess(text)\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd1d3d26-5056-4195-b21d-95a6d583252d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'quick',\n",
       " 'brown',\n",
       " 'fox',\n",
       " 'jumped',\n",
       " 'over',\n",
       " 'the',\n",
       " 'high',\n",
       " 'speed',\n",
       " 'fence']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyphenated Words\n",
    "# not good\n",
    "text = \"The quick brown fox jumped over the high-speed fence.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = simple_preprocess(text)\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b8e577-83b2-491b-9563-4d1903f8bc49",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d315bd5-c6ab-4132-8839-7b186e51166e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compare_list = ['https://t.co/9z2J3P33Uc',\n",
    "               'laugh/cry',\n",
    "               'ðŸ˜¬ðŸ˜­ðŸ˜“ðŸ¤¢ðŸ™„ðŸ˜±',\n",
    "               \"world's problems\",\n",
    "               \"@datageneral\",\n",
    "                \"It's interesting\",\n",
    "               \"don't spell my name right\",\n",
    "               'all-nighter',\n",
    "                \"My favorite color is blue\",\n",
    "                \"My favorite colors are blue, red, and green.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a85224b-74f1-44b5-ab00-7e0e567703b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https', 'co', 'uc']\n",
      "['laugh', 'cry']\n",
      "[]\n",
      "['world', 'problems']\n",
      "['datageneral']\n",
      "['it', 'interesting']\n",
      "['don', 'spell', 'my', 'name', 'right']\n",
      "['all', 'nighter']\n",
      "['my', 'favorite', 'color', 'is', 'blue']\n",
      "['my', 'favorite', 'colors', 'are', 'blue', 'red', 'and', 'green']\n"
     ]
    }
   ],
   "source": [
    "for sent in compare_list:\n",
    "    \n",
    "    # Tokenize the text into words\n",
    "    tokens = simple_preprocess(sent)\n",
    "\n",
    "    # Print tokens\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea9e9d0-b5f6-42ef-b6f4-7773380bda7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
