{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49ac3a04-2f86-4068-a74d-d9e13e340afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04d954c5-7d40-4b04-bfef-b9c66e345dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator Network\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(784, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), 784)\n",
    "        validity = self.model(x)\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d2dfd05-5dc5-4b03-ae2d-b7d9a4259e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator Network\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(100, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 784),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        img = self.model(x)\n",
    "        img = img.view(img.size(0), 1, 28, 28)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d010e00-a1c1-42ed-a3bb-0169a3cb1507",
   "metadata": {},
   "source": [
    "#### define the training loop for our GAN. \n",
    "\n",
    "- We'll train the discriminator and generator networks alternately. \n",
    "\n",
    "- In each iteration, we'll feed real and fake images to the discriminator, compute the losses, and update the networks accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6baf70b0-c29f-4548-9ce9-8e68e85eb307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the networks\n",
    "discriminator = Discriminator()\n",
    "generator     = Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e43d4d38-3e7e-4314-94b1-8dc35e2e20cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizers\n",
    "adversarial_loss = nn.BCELoss()\n",
    "\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002)\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3014f23a-10af-4a1f-83c1-b3f3219b5262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 200\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31546ad7-8cc6-4ae2-862d-85b217971352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "])\n",
    "\n",
    "dataset       = datasets.MNIST(root      = r\"E:\\AI-DATASETS\\pytorch\\mnist-train\", \n",
    "                                train    = True, \n",
    "                                transform= transform,\n",
    "                                download = True)\n",
    "\n",
    "#dataset    = datasets.MNIST(root='data', train=True, transform=transform, download=True)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7aae8dbb-4ae4-4c3a-a6b2-bd0b25a5cf9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/200], Batch Step [0/600], D_loss: 1.3756, G_loss: 0.6617\n",
      "Epoch [0/200], Batch Step [100/600], D_loss: 0.1239, G_loss: 2.8561\n",
      "Epoch [0/200], Batch Step [200/600], D_loss: 0.7386, G_loss: 1.0211\n",
      "Epoch [0/200], Batch Step [300/600], D_loss: 0.0309, G_loss: 4.6599\n",
      "Epoch [0/200], Batch Step [400/600], D_loss: 0.2189, G_loss: 5.9777\n",
      "Epoch [0/200], Batch Step [500/600], D_loss: 0.0260, G_loss: 5.1129\n",
      "Epoch [1/200], Batch Step [0/600], D_loss: 0.2660, G_loss: 3.7912\n",
      "Epoch [1/200], Batch Step [100/600], D_loss: 0.1805, G_loss: 4.1844\n",
      "Epoch [1/200], Batch Step [200/600], D_loss: 0.1618, G_loss: 5.4391\n",
      "Epoch [1/200], Batch Step [300/600], D_loss: 0.3433, G_loss: 2.8980\n",
      "Epoch [1/200], Batch Step [400/600], D_loss: 0.7820, G_loss: 1.5787\n",
      "Epoch [1/200], Batch Step [500/600], D_loss: 0.1943, G_loss: 3.0256\n",
      "Epoch [2/200], Batch Step [0/600], D_loss: 0.3072, G_loss: 3.4207\n",
      "Epoch [2/200], Batch Step [100/600], D_loss: 0.6036, G_loss: 2.6584\n",
      "Epoch [2/200], Batch Step [200/600], D_loss: 0.1136, G_loss: 5.3496\n",
      "Epoch [2/200], Batch Step [300/600], D_loss: 0.2924, G_loss: 3.6403\n",
      "Epoch [2/200], Batch Step [400/600], D_loss: 0.4760, G_loss: 3.5791\n",
      "Epoch [2/200], Batch Step [500/600], D_loss: 0.6579, G_loss: 2.1380\n",
      "Epoch [3/200], Batch Step [0/600], D_loss: 1.1708, G_loss: 1.3005\n",
      "Epoch [3/200], Batch Step [100/600], D_loss: 0.2907, G_loss: 4.0413\n",
      "Epoch [3/200], Batch Step [200/600], D_loss: 1.1706, G_loss: 1.7507\n",
      "Epoch [3/200], Batch Step [300/600], D_loss: 1.0474, G_loss: 2.3494\n",
      "Epoch [3/200], Batch Step [400/600], D_loss: 0.4638, G_loss: 4.1974\n",
      "Epoch [3/200], Batch Step [500/600], D_loss: 0.5802, G_loss: 2.6213\n",
      "Epoch [4/200], Batch Step [0/600], D_loss: 0.3126, G_loss: 4.0697\n",
      "Epoch [4/200], Batch Step [100/600], D_loss: 0.5108, G_loss: 2.0955\n",
      "Epoch [4/200], Batch Step [200/600], D_loss: 0.2528, G_loss: 2.5764\n",
      "Epoch [4/200], Batch Step [300/600], D_loss: 0.2508, G_loss: 4.4316\n",
      "Epoch [4/200], Batch Step [400/600], D_loss: 0.8632, G_loss: 1.8231\n",
      "Epoch [4/200], Batch Step [500/600], D_loss: 0.6729, G_loss: 3.5679\n",
      "Epoch [5/200], Batch Step [0/600], D_loss: 0.5510, G_loss: 3.2373\n",
      "Epoch [5/200], Batch Step [100/600], D_loss: 0.5924, G_loss: 3.5648\n",
      "Epoch [5/200], Batch Step [200/600], D_loss: 0.6584, G_loss: 3.1754\n",
      "Epoch [5/200], Batch Step [300/600], D_loss: 0.5742, G_loss: 2.8650\n",
      "Epoch [5/200], Batch Step [400/600], D_loss: 0.3479, G_loss: 2.8151\n",
      "Epoch [5/200], Batch Step [500/600], D_loss: 0.3579, G_loss: 4.6454\n",
      "Epoch [6/200], Batch Step [0/600], D_loss: 0.2383, G_loss: 3.1246\n",
      "Epoch [6/200], Batch Step [100/600], D_loss: 0.7177, G_loss: 3.8235\n",
      "Epoch [6/200], Batch Step [200/600], D_loss: 1.0504, G_loss: 3.8346\n",
      "Epoch [6/200], Batch Step [300/600], D_loss: 1.2319, G_loss: 3.0712\n",
      "Epoch [6/200], Batch Step [400/600], D_loss: 1.7886, G_loss: 3.1969\n",
      "Epoch [6/200], Batch Step [500/600], D_loss: 0.6613, G_loss: 3.3052\n",
      "Epoch [7/200], Batch Step [0/600], D_loss: 0.6921, G_loss: 2.7107\n",
      "Epoch [7/200], Batch Step [100/600], D_loss: 0.5379, G_loss: 3.5458\n",
      "Epoch [7/200], Batch Step [200/600], D_loss: 0.5785, G_loss: 1.9029\n",
      "Epoch [7/200], Batch Step [300/600], D_loss: 0.3981, G_loss: 2.9083\n",
      "Epoch [7/200], Batch Step [400/600], D_loss: 0.3469, G_loss: 3.2289\n",
      "Epoch [7/200], Batch Step [500/600], D_loss: 0.3584, G_loss: 4.1383\n",
      "Epoch [8/200], Batch Step [0/600], D_loss: 0.2907, G_loss: 4.2316\n",
      "Epoch [8/200], Batch Step [100/600], D_loss: 0.3986, G_loss: 2.1299\n",
      "Epoch [8/200], Batch Step [200/600], D_loss: 0.6022, G_loss: 2.0241\n",
      "Epoch [8/200], Batch Step [300/600], D_loss: 0.7992, G_loss: 1.4163\n",
      "Epoch [8/200], Batch Step [400/600], D_loss: 0.4546, G_loss: 3.8027\n",
      "Epoch [8/200], Batch Step [500/600], D_loss: 0.4147, G_loss: 3.4593\n",
      "Epoch [9/200], Batch Step [0/600], D_loss: 0.5832, G_loss: 4.2578\n",
      "Epoch [9/200], Batch Step [100/600], D_loss: 0.6849, G_loss: 3.1591\n",
      "Epoch [9/200], Batch Step [200/600], D_loss: 0.4798, G_loss: 2.8189\n",
      "Epoch [9/200], Batch Step [300/600], D_loss: 0.5242, G_loss: 3.2173\n",
      "Epoch [9/200], Batch Step [400/600], D_loss: 0.3057, G_loss: 3.2671\n",
      "Epoch [9/200], Batch Step [500/600], D_loss: 0.5599, G_loss: 3.0344\n",
      "Epoch [10/200], Batch Step [0/600], D_loss: 0.3703, G_loss: 2.8619\n",
      "Epoch [10/200], Batch Step [100/600], D_loss: 0.3834, G_loss: 2.5166\n",
      "Epoch [10/200], Batch Step [200/600], D_loss: 0.6284, G_loss: 1.6912\n",
      "Epoch [10/200], Batch Step [300/600], D_loss: 0.3897, G_loss: 3.2271\n",
      "Epoch [10/200], Batch Step [400/600], D_loss: 0.5986, G_loss: 2.7640\n",
      "Epoch [10/200], Batch Step [500/600], D_loss: 1.0088, G_loss: 2.2470\n",
      "Epoch [11/200], Batch Step [0/600], D_loss: 0.7654, G_loss: 2.7356\n",
      "Epoch [11/200], Batch Step [100/600], D_loss: 0.3374, G_loss: 3.2571\n",
      "Epoch [11/200], Batch Step [200/600], D_loss: 0.8259, G_loss: 2.8207\n",
      "Epoch [11/200], Batch Step [300/600], D_loss: 0.7531, G_loss: 2.2340\n",
      "Epoch [11/200], Batch Step [400/600], D_loss: 0.4392, G_loss: 2.1573\n",
      "Epoch [11/200], Batch Step [500/600], D_loss: 0.2920, G_loss: 2.8463\n",
      "Epoch [12/200], Batch Step [0/600], D_loss: 0.6167, G_loss: 3.0407\n",
      "Epoch [12/200], Batch Step [100/600], D_loss: 0.2993, G_loss: 3.1300\n",
      "Epoch [12/200], Batch Step [200/600], D_loss: 0.3791, G_loss: 2.2428\n",
      "Epoch [12/200], Batch Step [300/600], D_loss: 0.3834, G_loss: 3.4233\n",
      "Epoch [12/200], Batch Step [400/600], D_loss: 0.1618, G_loss: 3.3792\n",
      "Epoch [12/200], Batch Step [500/600], D_loss: 0.3408, G_loss: 4.5428\n",
      "Epoch [13/200], Batch Step [0/600], D_loss: 0.2915, G_loss: 4.8497\n",
      "Epoch [13/200], Batch Step [100/600], D_loss: 0.4000, G_loss: 4.2509\n",
      "Epoch [13/200], Batch Step [200/600], D_loss: 0.3112, G_loss: 4.7593\n",
      "Epoch [13/200], Batch Step [300/600], D_loss: 0.2518, G_loss: 5.3943\n",
      "Epoch [13/200], Batch Step [400/600], D_loss: 0.2663, G_loss: 4.8170\n",
      "Epoch [13/200], Batch Step [500/600], D_loss: 0.2455, G_loss: 5.4279\n",
      "Epoch [14/200], Batch Step [0/600], D_loss: 0.3818, G_loss: 3.4369\n",
      "Epoch [14/200], Batch Step [100/600], D_loss: 0.2050, G_loss: 3.8356\n",
      "Epoch [14/200], Batch Step [200/600], D_loss: 0.3934, G_loss: 3.4498\n",
      "Epoch [14/200], Batch Step [300/600], D_loss: 0.4329, G_loss: 4.8178\n",
      "Epoch [14/200], Batch Step [400/600], D_loss: 0.4038, G_loss: 3.4202\n",
      "Epoch [14/200], Batch Step [500/600], D_loss: 0.3538, G_loss: 3.9082\n",
      "Epoch [15/200], Batch Step [0/600], D_loss: 0.4552, G_loss: 3.1189\n",
      "Epoch [15/200], Batch Step [100/600], D_loss: 0.4441, G_loss: 3.5566\n",
      "Epoch [15/200], Batch Step [200/600], D_loss: 0.3338, G_loss: 3.2875\n",
      "Epoch [15/200], Batch Step [300/600], D_loss: 0.4725, G_loss: 2.8461\n",
      "Epoch [15/200], Batch Step [400/600], D_loss: 0.3745, G_loss: 3.3545\n",
      "Epoch [15/200], Batch Step [500/600], D_loss: 0.5354, G_loss: 4.3863\n",
      "Epoch [16/200], Batch Step [0/600], D_loss: 0.6531, G_loss: 3.5081\n",
      "Epoch [16/200], Batch Step [100/600], D_loss: 0.2648, G_loss: 4.9693\n",
      "Epoch [16/200], Batch Step [200/600], D_loss: 0.3634, G_loss: 4.6376\n",
      "Epoch [16/200], Batch Step [300/600], D_loss: 0.2290, G_loss: 5.2191\n",
      "Epoch [16/200], Batch Step [400/600], D_loss: 0.2291, G_loss: 5.1599\n",
      "Epoch [16/200], Batch Step [500/600], D_loss: 0.6477, G_loss: 4.3849\n",
      "Epoch [17/200], Batch Step [0/600], D_loss: 0.5705, G_loss: 2.2244\n",
      "Epoch [17/200], Batch Step [100/600], D_loss: 0.6542, G_loss: 4.0356\n",
      "Epoch [17/200], Batch Step [200/600], D_loss: 0.5927, G_loss: 3.5502\n",
      "Epoch [17/200], Batch Step [300/600], D_loss: 0.3909, G_loss: 3.2987\n",
      "Epoch [17/200], Batch Step [400/600], D_loss: 0.3220, G_loss: 3.3437\n",
      "Epoch [17/200], Batch Step [500/600], D_loss: 0.3026, G_loss: 3.4856\n",
      "Epoch [18/200], Batch Step [0/600], D_loss: 0.3347, G_loss: 2.5303\n",
      "Epoch [18/200], Batch Step [100/600], D_loss: 0.5442, G_loss: 2.6862\n",
      "Epoch [18/200], Batch Step [200/600], D_loss: 0.2913, G_loss: 3.5604\n",
      "Epoch [18/200], Batch Step [300/600], D_loss: 0.5732, G_loss: 3.4422\n",
      "Epoch [18/200], Batch Step [400/600], D_loss: 0.6710, G_loss: 3.9128\n",
      "Epoch [18/200], Batch Step [500/600], D_loss: 0.3044, G_loss: 4.3387\n",
      "Epoch [19/200], Batch Step [0/600], D_loss: 0.4858, G_loss: 4.6128\n",
      "Epoch [19/200], Batch Step [100/600], D_loss: 0.1846, G_loss: 4.7878\n",
      "Epoch [19/200], Batch Step [200/600], D_loss: 0.5834, G_loss: 4.2995\n",
      "Epoch [19/200], Batch Step [300/600], D_loss: 0.3156, G_loss: 3.1678\n",
      "Epoch [19/200], Batch Step [400/600], D_loss: 0.5465, G_loss: 4.1410\n",
      "Epoch [19/200], Batch Step [500/600], D_loss: 0.5165, G_loss: 3.4223\n",
      "Epoch [20/200], Batch Step [0/600], D_loss: 0.3888, G_loss: 4.2365\n",
      "Epoch [20/200], Batch Step [100/600], D_loss: 0.6064, G_loss: 3.7127\n",
      "Epoch [20/200], Batch Step [200/600], D_loss: 0.3894, G_loss: 4.1897\n",
      "Epoch [20/200], Batch Step [300/600], D_loss: 0.5128, G_loss: 2.8411\n",
      "Epoch [20/200], Batch Step [400/600], D_loss: 0.5270, G_loss: 2.6051\n",
      "Epoch [20/200], Batch Step [500/600], D_loss: 0.4480, G_loss: 3.8336\n",
      "Epoch [21/200], Batch Step [0/600], D_loss: 0.2430, G_loss: 3.7528\n",
      "Epoch [21/200], Batch Step [100/600], D_loss: 0.5099, G_loss: 4.3583\n",
      "Epoch [21/200], Batch Step [200/600], D_loss: 0.3781, G_loss: 3.2858\n",
      "Epoch [21/200], Batch Step [300/600], D_loss: 0.3142, G_loss: 3.5496\n",
      "Epoch [21/200], Batch Step [400/600], D_loss: 0.4109, G_loss: 4.0093\n",
      "Epoch [21/200], Batch Step [500/600], D_loss: 0.4972, G_loss: 3.5056\n",
      "Epoch [22/200], Batch Step [0/600], D_loss: 0.3170, G_loss: 3.8679\n",
      "Epoch [22/200], Batch Step [100/600], D_loss: 0.6127, G_loss: 4.2840\n",
      "Epoch [22/200], Batch Step [200/600], D_loss: 0.3136, G_loss: 3.7166\n",
      "Epoch [22/200], Batch Step [300/600], D_loss: 0.2018, G_loss: 3.3697\n",
      "Epoch [22/200], Batch Step [400/600], D_loss: 0.2809, G_loss: 4.7863\n",
      "Epoch [22/200], Batch Step [500/600], D_loss: 0.6177, G_loss: 2.4930\n",
      "Epoch [23/200], Batch Step [0/600], D_loss: 0.4230, G_loss: 3.5953\n",
      "Epoch [23/200], Batch Step [100/600], D_loss: 0.3404, G_loss: 3.7595\n",
      "Epoch [23/200], Batch Step [200/600], D_loss: 0.4254, G_loss: 4.6636\n",
      "Epoch [23/200], Batch Step [300/600], D_loss: 0.4464, G_loss: 3.4747\n",
      "Epoch [23/200], Batch Step [400/600], D_loss: 0.3667, G_loss: 3.0747\n",
      "Epoch [23/200], Batch Step [500/600], D_loss: 0.4292, G_loss: 3.7350\n",
      "Epoch [24/200], Batch Step [0/600], D_loss: 0.5219, G_loss: 2.6925\n",
      "Epoch [24/200], Batch Step [100/600], D_loss: 0.4531, G_loss: 5.1512\n",
      "Epoch [24/200], Batch Step [200/600], D_loss: 0.2999, G_loss: 3.9623\n",
      "Epoch [24/200], Batch Step [300/600], D_loss: 0.3123, G_loss: 3.7932\n",
      "Epoch [24/200], Batch Step [400/600], D_loss: 0.3596, G_loss: 4.9961\n",
      "Epoch [24/200], Batch Step [500/600], D_loss: 0.2959, G_loss: 2.7673\n",
      "Epoch [25/200], Batch Step [0/600], D_loss: 0.5275, G_loss: 3.5059\n",
      "Epoch [25/200], Batch Step [100/600], D_loss: 0.4047, G_loss: 3.3965\n",
      "Epoch [25/200], Batch Step [200/600], D_loss: 0.5724, G_loss: 2.8138\n",
      "Epoch [25/200], Batch Step [300/600], D_loss: 0.3239, G_loss: 3.3800\n",
      "Epoch [25/200], Batch Step [400/600], D_loss: 0.4606, G_loss: 3.7704\n",
      "Epoch [25/200], Batch Step [500/600], D_loss: 0.3415, G_loss: 2.7950\n",
      "Epoch [26/200], Batch Step [0/600], D_loss: 0.4327, G_loss: 3.0677\n",
      "Epoch [26/200], Batch Step [100/600], D_loss: 0.3960, G_loss: 2.7951\n",
      "Epoch [26/200], Batch Step [200/600], D_loss: 0.4194, G_loss: 3.2707\n",
      "Epoch [26/200], Batch Step [300/600], D_loss: 0.4039, G_loss: 3.6524\n",
      "Epoch [26/200], Batch Step [400/600], D_loss: 0.4723, G_loss: 3.0996\n",
      "Epoch [26/200], Batch Step [500/600], D_loss: 0.5211, G_loss: 2.7187\n",
      "Epoch [27/200], Batch Step [0/600], D_loss: 0.3626, G_loss: 3.7751\n",
      "Epoch [27/200], Batch Step [100/600], D_loss: 0.5215, G_loss: 2.9706\n",
      "Epoch [27/200], Batch Step [200/600], D_loss: 0.4526, G_loss: 2.6995\n",
      "Epoch [27/200], Batch Step [300/600], D_loss: 0.6005, G_loss: 2.6750\n",
      "Epoch [27/200], Batch Step [400/600], D_loss: 0.5331, G_loss: 2.3843\n",
      "Epoch [27/200], Batch Step [500/600], D_loss: 0.3446, G_loss: 3.3909\n",
      "Epoch [28/200], Batch Step [0/600], D_loss: 0.4623, G_loss: 3.0810\n",
      "Epoch [28/200], Batch Step [100/600], D_loss: 0.2531, G_loss: 3.7064\n",
      "Epoch [28/200], Batch Step [200/600], D_loss: 0.4255, G_loss: 2.8488\n",
      "Epoch [28/200], Batch Step [300/600], D_loss: 0.3269, G_loss: 2.8924\n",
      "Epoch [28/200], Batch Step [400/600], D_loss: 0.4828, G_loss: 3.2785\n",
      "Epoch [28/200], Batch Step [500/600], D_loss: 0.3205, G_loss: 3.0179\n",
      "Epoch [29/200], Batch Step [0/600], D_loss: 0.5035, G_loss: 2.9801\n",
      "Epoch [29/200], Batch Step [100/600], D_loss: 0.4323, G_loss: 2.9451\n",
      "Epoch [29/200], Batch Step [200/600], D_loss: 0.4372, G_loss: 3.4445\n",
      "Epoch [29/200], Batch Step [300/600], D_loss: 0.5958, G_loss: 3.0990\n",
      "Epoch [29/200], Batch Step [400/600], D_loss: 0.5900, G_loss: 2.0438\n",
      "Epoch [29/200], Batch Step [500/600], D_loss: 0.5801, G_loss: 3.1669\n",
      "Epoch [30/200], Batch Step [0/600], D_loss: 0.4278, G_loss: 3.7398\n",
      "Epoch [30/200], Batch Step [100/600], D_loss: 0.3535, G_loss: 3.6198\n",
      "Epoch [30/200], Batch Step [200/600], D_loss: 0.7770, G_loss: 2.6528\n",
      "Epoch [30/200], Batch Step [300/600], D_loss: 0.5277, G_loss: 3.5834\n",
      "Epoch [30/200], Batch Step [400/600], D_loss: 0.4111, G_loss: 3.5086\n",
      "Epoch [30/200], Batch Step [500/600], D_loss: 0.4564, G_loss: 2.8498\n",
      "Epoch [31/200], Batch Step [0/600], D_loss: 0.6351, G_loss: 3.0606\n",
      "Epoch [31/200], Batch Step [100/600], D_loss: 0.4018, G_loss: 2.1291\n",
      "Epoch [31/200], Batch Step [200/600], D_loss: 0.4321, G_loss: 3.0153\n",
      "Epoch [31/200], Batch Step [300/600], D_loss: 0.5433, G_loss: 3.1941\n",
      "Epoch [31/200], Batch Step [400/600], D_loss: 0.6038, G_loss: 2.1059\n",
      "Epoch [31/200], Batch Step [500/600], D_loss: 0.7536, G_loss: 2.7598\n",
      "Epoch [32/200], Batch Step [0/600], D_loss: 0.4655, G_loss: 2.2224\n",
      "Epoch [32/200], Batch Step [100/600], D_loss: 0.3086, G_loss: 3.0773\n",
      "Epoch [32/200], Batch Step [200/600], D_loss: 0.4458, G_loss: 2.6239\n",
      "Epoch [32/200], Batch Step [300/600], D_loss: 0.4941, G_loss: 2.2202\n",
      "Epoch [32/200], Batch Step [400/600], D_loss: 0.4268, G_loss: 2.8569\n",
      "Epoch [32/200], Batch Step [500/600], D_loss: 0.6591, G_loss: 2.5299\n",
      "Epoch [33/200], Batch Step [0/600], D_loss: 0.3617, G_loss: 2.6058\n",
      "Epoch [33/200], Batch Step [100/600], D_loss: 0.4530, G_loss: 2.5540\n",
      "Epoch [33/200], Batch Step [200/600], D_loss: 0.4925, G_loss: 2.9699\n",
      "Epoch [33/200], Batch Step [300/600], D_loss: 0.5346, G_loss: 3.3731\n",
      "Epoch [33/200], Batch Step [400/600], D_loss: 0.6002, G_loss: 2.7042\n",
      "Epoch [33/200], Batch Step [500/600], D_loss: 0.5739, G_loss: 3.1923\n",
      "Epoch [34/200], Batch Step [0/600], D_loss: 0.6079, G_loss: 3.4042\n",
      "Epoch [34/200], Batch Step [100/600], D_loss: 0.4893, G_loss: 2.8448\n",
      "Epoch [34/200], Batch Step [200/600], D_loss: 0.4207, G_loss: 2.9773\n",
      "Epoch [34/200], Batch Step [300/600], D_loss: 0.4991, G_loss: 2.5442\n",
      "Epoch [34/200], Batch Step [400/600], D_loss: 0.4818, G_loss: 2.0301\n",
      "Epoch [34/200], Batch Step [500/600], D_loss: 0.6250, G_loss: 1.9621\n",
      "Epoch [35/200], Batch Step [0/600], D_loss: 0.5221, G_loss: 3.7760\n",
      "Epoch [35/200], Batch Step [100/600], D_loss: 0.3682, G_loss: 2.5378\n",
      "Epoch [35/200], Batch Step [200/600], D_loss: 0.5749, G_loss: 2.4234\n",
      "Epoch [35/200], Batch Step [300/600], D_loss: 0.6282, G_loss: 2.2977\n",
      "Epoch [35/200], Batch Step [400/600], D_loss: 0.4947, G_loss: 2.3021\n",
      "Epoch [35/200], Batch Step [500/600], D_loss: 0.5900, G_loss: 3.0912\n",
      "Epoch [36/200], Batch Step [0/600], D_loss: 0.6574, G_loss: 1.8509\n",
      "Epoch [36/200], Batch Step [100/600], D_loss: 0.6181, G_loss: 2.4694\n",
      "Epoch [36/200], Batch Step [200/600], D_loss: 0.5928, G_loss: 2.9451\n",
      "Epoch [36/200], Batch Step [300/600], D_loss: 0.6843, G_loss: 2.1772\n",
      "Epoch [36/200], Batch Step [400/600], D_loss: 0.4837, G_loss: 2.3160\n",
      "Epoch [36/200], Batch Step [500/600], D_loss: 0.6030, G_loss: 2.4272\n",
      "Epoch [37/200], Batch Step [0/600], D_loss: 0.5147, G_loss: 2.6502\n",
      "Epoch [37/200], Batch Step [100/600], D_loss: 0.6206, G_loss: 1.8633\n",
      "Epoch [37/200], Batch Step [200/600], D_loss: 0.6326, G_loss: 2.2694\n",
      "Epoch [37/200], Batch Step [300/600], D_loss: 0.7052, G_loss: 2.6937\n",
      "Epoch [37/200], Batch Step [400/600], D_loss: 0.3670, G_loss: 2.7940\n",
      "Epoch [37/200], Batch Step [500/600], D_loss: 0.5711, G_loss: 2.6345\n",
      "Epoch [38/200], Batch Step [0/600], D_loss: 0.4650, G_loss: 2.8542\n",
      "Epoch [38/200], Batch Step [100/600], D_loss: 0.6523, G_loss: 2.5286\n",
      "Epoch [38/200], Batch Step [200/600], D_loss: 0.5445, G_loss: 2.4787\n",
      "Epoch [38/200], Batch Step [300/600], D_loss: 0.4533, G_loss: 3.2162\n",
      "Epoch [38/200], Batch Step [400/600], D_loss: 0.5900, G_loss: 2.3724\n",
      "Epoch [38/200], Batch Step [500/600], D_loss: 0.5146, G_loss: 2.2213\n",
      "Epoch [39/200], Batch Step [0/600], D_loss: 0.6861, G_loss: 2.0126\n",
      "Epoch [39/200], Batch Step [100/600], D_loss: 0.8559, G_loss: 1.9344\n",
      "Epoch [39/200], Batch Step [200/600], D_loss: 0.5298, G_loss: 2.5391\n",
      "Epoch [39/200], Batch Step [300/600], D_loss: 0.5513, G_loss: 2.1148\n",
      "Epoch [39/200], Batch Step [400/600], D_loss: 0.5914, G_loss: 2.5648\n",
      "Epoch [39/200], Batch Step [500/600], D_loss: 0.6263, G_loss: 1.9881\n",
      "Epoch [40/200], Batch Step [0/600], D_loss: 0.5468, G_loss: 2.9013\n",
      "Epoch [40/200], Batch Step [100/600], D_loss: 0.4703, G_loss: 2.4013\n",
      "Epoch [40/200], Batch Step [200/600], D_loss: 0.5910, G_loss: 2.4362\n",
      "Epoch [40/200], Batch Step [300/600], D_loss: 0.5920, G_loss: 2.4733\n",
      "Epoch [40/200], Batch Step [400/600], D_loss: 0.4232, G_loss: 2.8326\n",
      "Epoch [40/200], Batch Step [500/600], D_loss: 0.6924, G_loss: 2.1012\n",
      "Epoch [41/200], Batch Step [0/600], D_loss: 0.5551, G_loss: 1.9050\n",
      "Epoch [41/200], Batch Step [100/600], D_loss: 0.5994, G_loss: 2.8659\n",
      "Epoch [41/200], Batch Step [200/600], D_loss: 0.5727, G_loss: 2.4246\n",
      "Epoch [41/200], Batch Step [300/600], D_loss: 0.5765, G_loss: 2.9857\n",
      "Epoch [41/200], Batch Step [400/600], D_loss: 0.9172, G_loss: 1.5271\n",
      "Epoch [41/200], Batch Step [500/600], D_loss: 0.7473, G_loss: 2.0726\n",
      "Epoch [42/200], Batch Step [0/600], D_loss: 0.7335, G_loss: 2.0527\n",
      "Epoch [42/200], Batch Step [100/600], D_loss: 0.8531, G_loss: 1.6884\n",
      "Epoch [42/200], Batch Step [200/600], D_loss: 0.6495, G_loss: 2.6863\n",
      "Epoch [42/200], Batch Step [300/600], D_loss: 0.6407, G_loss: 1.9368\n",
      "Epoch [42/200], Batch Step [400/600], D_loss: 0.7277, G_loss: 1.8744\n",
      "Epoch [42/200], Batch Step [500/600], D_loss: 0.5621, G_loss: 2.4244\n",
      "Epoch [43/200], Batch Step [0/600], D_loss: 0.8980, G_loss: 1.8329\n",
      "Epoch [43/200], Batch Step [100/600], D_loss: 0.6810, G_loss: 2.3217\n",
      "Epoch [43/200], Batch Step [200/600], D_loss: 0.5389, G_loss: 1.9714\n",
      "Epoch [43/200], Batch Step [300/600], D_loss: 0.7147, G_loss: 2.3866\n",
      "Epoch [43/200], Batch Step [400/600], D_loss: 0.6390, G_loss: 1.9724\n",
      "Epoch [43/200], Batch Step [500/600], D_loss: 0.7029, G_loss: 2.4822\n",
      "Epoch [44/200], Batch Step [0/600], D_loss: 0.6723, G_loss: 1.8443\n",
      "Epoch [44/200], Batch Step [100/600], D_loss: 0.5407, G_loss: 2.6645\n",
      "Epoch [44/200], Batch Step [200/600], D_loss: 0.6489, G_loss: 1.8329\n",
      "Epoch [44/200], Batch Step [300/600], D_loss: 0.5200, G_loss: 2.4948\n",
      "Epoch [44/200], Batch Step [400/600], D_loss: 0.8555, G_loss: 1.8597\n",
      "Epoch [44/200], Batch Step [500/600], D_loss: 0.7707, G_loss: 2.2148\n",
      "Epoch [45/200], Batch Step [0/600], D_loss: 0.6812, G_loss: 1.8719\n",
      "Epoch [45/200], Batch Step [100/600], D_loss: 0.5258, G_loss: 2.2061\n",
      "Epoch [45/200], Batch Step [200/600], D_loss: 0.6535, G_loss: 1.9180\n",
      "Epoch [45/200], Batch Step [300/600], D_loss: 0.7664, G_loss: 2.4246\n",
      "Epoch [45/200], Batch Step [400/600], D_loss: 0.6437, G_loss: 1.3797\n",
      "Epoch [45/200], Batch Step [500/600], D_loss: 0.8752, G_loss: 2.0996\n",
      "Epoch [46/200], Batch Step [0/600], D_loss: 0.7113, G_loss: 1.7749\n",
      "Epoch [46/200], Batch Step [100/600], D_loss: 0.6648, G_loss: 1.9914\n",
      "Epoch [46/200], Batch Step [200/600], D_loss: 0.9613, G_loss: 1.8695\n",
      "Epoch [46/200], Batch Step [300/600], D_loss: 0.7914, G_loss: 1.7303\n",
      "Epoch [46/200], Batch Step [400/600], D_loss: 0.8206, G_loss: 2.0622\n",
      "Epoch [46/200], Batch Step [500/600], D_loss: 0.4928, G_loss: 2.1781\n",
      "Epoch [47/200], Batch Step [0/600], D_loss: 0.6860, G_loss: 1.8748\n",
      "Epoch [47/200], Batch Step [100/600], D_loss: 0.6918, G_loss: 2.0535\n",
      "Epoch [47/200], Batch Step [200/600], D_loss: 0.6096, G_loss: 2.2555\n",
      "Epoch [47/200], Batch Step [300/600], D_loss: 0.8049, G_loss: 1.9316\n",
      "Epoch [47/200], Batch Step [400/600], D_loss: 0.8050, G_loss: 1.7299\n",
      "Epoch [47/200], Batch Step [500/600], D_loss: 0.5236, G_loss: 2.5248\n",
      "Epoch [48/200], Batch Step [0/600], D_loss: 0.7979, G_loss: 1.9706\n",
      "Epoch [48/200], Batch Step [100/600], D_loss: 0.7286, G_loss: 1.7486\n",
      "Epoch [48/200], Batch Step [200/600], D_loss: 0.5659, G_loss: 2.5459\n",
      "Epoch [48/200], Batch Step [300/600], D_loss: 0.6647, G_loss: 1.8075\n",
      "Epoch [48/200], Batch Step [400/600], D_loss: 0.7857, G_loss: 2.0533\n",
      "Epoch [48/200], Batch Step [500/600], D_loss: 0.8973, G_loss: 1.6910\n",
      "Epoch [49/200], Batch Step [0/600], D_loss: 0.8428, G_loss: 1.7912\n",
      "Epoch [49/200], Batch Step [100/600], D_loss: 0.5260, G_loss: 1.9755\n",
      "Epoch [49/200], Batch Step [200/600], D_loss: 0.5618, G_loss: 2.3408\n",
      "Epoch [49/200], Batch Step [300/600], D_loss: 0.5754, G_loss: 2.3466\n",
      "Epoch [49/200], Batch Step [400/600], D_loss: 0.7024, G_loss: 2.0593\n",
      "Epoch [49/200], Batch Step [500/600], D_loss: 1.0749, G_loss: 1.5448\n",
      "Epoch [50/200], Batch Step [0/600], D_loss: 0.6823, G_loss: 2.0642\n",
      "Epoch [50/200], Batch Step [100/600], D_loss: 0.6881, G_loss: 2.0331\n",
      "Epoch [50/200], Batch Step [200/600], D_loss: 0.5993, G_loss: 2.1612\n",
      "Epoch [50/200], Batch Step [300/600], D_loss: 0.6501, G_loss: 2.0612\n",
      "Epoch [50/200], Batch Step [400/600], D_loss: 0.8111, G_loss: 1.7324\n",
      "Epoch [50/200], Batch Step [500/600], D_loss: 0.7396, G_loss: 2.3649\n",
      "Epoch [51/200], Batch Step [0/600], D_loss: 0.6915, G_loss: 2.5598\n",
      "Epoch [51/200], Batch Step [100/600], D_loss: 0.7342, G_loss: 2.0067\n",
      "Epoch [51/200], Batch Step [200/600], D_loss: 0.8346, G_loss: 1.3726\n",
      "Epoch [51/200], Batch Step [300/600], D_loss: 0.6308, G_loss: 2.3118\n",
      "Epoch [51/200], Batch Step [400/600], D_loss: 0.6112, G_loss: 2.1327\n",
      "Epoch [51/200], Batch Step [500/600], D_loss: 0.6645, G_loss: 1.7036\n",
      "Epoch [52/200], Batch Step [0/600], D_loss: 0.7591, G_loss: 2.4669\n",
      "Epoch [52/200], Batch Step [100/600], D_loss: 0.8691, G_loss: 1.4609\n",
      "Epoch [52/200], Batch Step [200/600], D_loss: 0.9952, G_loss: 1.5652\n",
      "Epoch [52/200], Batch Step [300/600], D_loss: 0.7263, G_loss: 1.8723\n",
      "Epoch [52/200], Batch Step [400/600], D_loss: 0.8600, G_loss: 1.7099\n",
      "Epoch [52/200], Batch Step [500/600], D_loss: 0.7758, G_loss: 1.8338\n",
      "Epoch [53/200], Batch Step [0/600], D_loss: 0.5974, G_loss: 2.2288\n",
      "Epoch [53/200], Batch Step [100/600], D_loss: 0.6868, G_loss: 1.9876\n",
      "Epoch [53/200], Batch Step [200/600], D_loss: 0.7333, G_loss: 1.7310\n",
      "Epoch [53/200], Batch Step [300/600], D_loss: 0.5876, G_loss: 2.3129\n",
      "Epoch [53/200], Batch Step [400/600], D_loss: 0.6156, G_loss: 2.1344\n",
      "Epoch [53/200], Batch Step [500/600], D_loss: 0.7436, G_loss: 2.2360\n",
      "Epoch [54/200], Batch Step [0/600], D_loss: 0.8864, G_loss: 1.5233\n",
      "Epoch [54/200], Batch Step [100/600], D_loss: 0.6224, G_loss: 2.1583\n",
      "Epoch [54/200], Batch Step [200/600], D_loss: 0.5461, G_loss: 2.1089\n",
      "Epoch [54/200], Batch Step [300/600], D_loss: 0.7162, G_loss: 1.9758\n",
      "Epoch [54/200], Batch Step [400/600], D_loss: 0.8051, G_loss: 2.0591\n",
      "Epoch [54/200], Batch Step [500/600], D_loss: 0.6319, G_loss: 2.2408\n",
      "Epoch [55/200], Batch Step [0/600], D_loss: 0.6958, G_loss: 1.7081\n",
      "Epoch [55/200], Batch Step [100/600], D_loss: 0.9218, G_loss: 2.0426\n",
      "Epoch [55/200], Batch Step [200/600], D_loss: 0.5396, G_loss: 2.2847\n",
      "Epoch [55/200], Batch Step [300/600], D_loss: 0.6358, G_loss: 2.0120\n",
      "Epoch [55/200], Batch Step [400/600], D_loss: 0.9502, G_loss: 1.9130\n",
      "Epoch [55/200], Batch Step [500/600], D_loss: 0.7287, G_loss: 2.1848\n",
      "Epoch [56/200], Batch Step [0/600], D_loss: 0.8613, G_loss: 1.7915\n",
      "Epoch [56/200], Batch Step [100/600], D_loss: 0.7211, G_loss: 1.9483\n",
      "Epoch [56/200], Batch Step [200/600], D_loss: 0.4540, G_loss: 2.4176\n",
      "Epoch [56/200], Batch Step [300/600], D_loss: 0.6275, G_loss: 1.9197\n",
      "Epoch [56/200], Batch Step [400/600], D_loss: 0.7586, G_loss: 1.9627\n",
      "Epoch [56/200], Batch Step [500/600], D_loss: 0.8294, G_loss: 1.6944\n",
      "Epoch [57/200], Batch Step [0/600], D_loss: 0.7514, G_loss: 1.6267\n",
      "Epoch [57/200], Batch Step [100/600], D_loss: 0.6414, G_loss: 2.0693\n",
      "Epoch [57/200], Batch Step [200/600], D_loss: 0.7503, G_loss: 1.8954\n",
      "Epoch [57/200], Batch Step [300/600], D_loss: 0.6161, G_loss: 2.1421\n",
      "Epoch [57/200], Batch Step [400/600], D_loss: 0.7498, G_loss: 1.7033\n",
      "Epoch [57/200], Batch Step [500/600], D_loss: 0.7174, G_loss: 1.6151\n",
      "Epoch [58/200], Batch Step [0/600], D_loss: 0.7346, G_loss: 1.6448\n",
      "Epoch [58/200], Batch Step [100/600], D_loss: 0.5804, G_loss: 2.3326\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Fake images\u001b[39;00m\n\u001b[0;32m     16\u001b[0m z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(real_images\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m fake_images \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m fake_pred \u001b[38;5;241m=\u001b[39m discriminator(fake_images\u001b[38;5;241m.\u001b[39mdetach())\n\u001b[0;32m     19\u001b[0m d_fake_loss \u001b[38;5;241m=\u001b[39m adversarial_loss(fake_pred, fake)\n",
      "File \u001b[1;32mD:\\ANACONDA\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[5], line 15\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 15\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mview(img\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m)\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mD:\\ANACONDA\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\ANACONDA\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mD:\\ANACONDA\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\ANACONDA\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # for a batch of 100\n",
    "    for i, (real_images, _) in enumerate(dataloader):\n",
    "        \n",
    "        # Adversarial ground truth labels\n",
    "        valid = torch.ones(real_images.size(0),  1)\n",
    "        fake  = torch.zeros(real_images.size(0), 1)\n",
    "\n",
    "        # Train the discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Real images\n",
    "        real_images = real_images.view(real_images.size(0), -1)\n",
    "        real_pred   = discriminator(real_images)\n",
    "        d_real_loss = adversarial_loss(real_pred, valid)\n",
    "\n",
    "        # Fake images\n",
    "        z = torch.randn(real_images.size(0), 100)\n",
    "        fake_images = generator(z)\n",
    "        fake_pred   = discriminator(fake_images.detach())\n",
    "        d_fake_loss = adversarial_loss(fake_pred, fake)\n",
    "\n",
    "        # Total discriminator loss\n",
    "        d_loss = d_real_loss + d_fake_loss\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Train the generator\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        z           = torch.randn(real_images.size(0), 100)\n",
    "        fake_images = generator(z)\n",
    "        fake_pred   = discriminator(fake_images)\n",
    "        g_loss      = adversarial_loss(fake_pred, valid)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{num_epochs}], Batch Step [{i}/{len(dataloader)}],\"\n",
    "                f\" D_loss: {d_loss.item():.4f}, G_loss: {g_loss.item():.4f}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8dbaf8-7b0f-4ce6-a9e3-d53eed3af51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ...\n",
    "\n",
    "# After the training loop\n",
    "\n",
    "# Generate and visualize fake images\n",
    "with torch.no_grad():\n",
    "    num_samples = 10\n",
    "    z = torch.randn(num_samples, 100)\n",
    "    fake_images = generator(z).detach()\n",
    "\n",
    "    fake_images = fake_images.view(-1, 28, 28)\n",
    "    fake_images = fake_images * 0.5 + 0.5  # Denormalize the images\n",
    "    fake_images = fake_images.numpy()\n",
    "\n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(num_samples, 1))\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.axis('off')\n",
    "        ax.imshow(fake_images[i], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282a4612-61e4-49d6-a01b-77441b40866a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the GAN model\n",
    "# torch.save({\n",
    "#     'generator_state_dict'    : generator.state_dict(),\n",
    "#     'discriminator_state_dict': discriminator.state_dict(),\n",
    "#     'optimizer_G_state_dict'  : optimizer_G.state_dict(),\n",
    "#     'optimizer_D_state_dict'  : optimizer_D.state_dict()\n",
    "# }, 'gan_model_mnist.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954d1ea8-17c9-435a-95c3-7666a584877a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the GAN model\n",
    "torch.save( {\n",
    "            'generator_state_dict':     generator.state_dict(),\n",
    "            'discriminator_state_dict': discriminator.state_dict(),\n",
    "            }, \n",
    "    'gan_model_mnist.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf35c960-5d4f-49c9-ba20-18dc2800fb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GAN model\n",
    "checkpoint = torch.load('gan_model_mnist.pth')\n",
    "generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "optimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])\n",
    "optimizer_D.load_state_dict(checkpoint['optimizer_D_state_dict'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
