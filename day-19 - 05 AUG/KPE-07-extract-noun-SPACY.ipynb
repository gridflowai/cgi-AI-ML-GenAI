{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4292c827-a502-4310-a4f5-5e6717a5d1e6",
   "metadata": {},
   "source": [
    "-------------------\n",
    "#### Extracting Noun Chunks - using spacy\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e4f310e-8a70-4a67-8d09-c7cfc192a13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8397bc31-bf9f-4fa1-8a26-aa2ede1680d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4fc59ab9-f656-4ba6-9e06-4bf0ed70f0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(filename):\n",
    "    file = open(filename, \"r\", encoding=\"utf-8\") \n",
    "    return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff35305a-2ce9-403b-80b8-6c477facdfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "location = r'D:\\AI-DATASETS\\01-MISC\\Sherlock-Holmes.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7bf8d12b-4615-4a6f-8a9a-a3ee5eca8866",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = read_text_file(location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b398d9-b229-42da-9f96-dea86c4521d9",
   "metadata": {},
   "source": [
    "Initialize the spacy engine and then use it to process the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e68e2ba2-a746-4e50-87cf-90449e5314bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22175773-1ff6-4a18-8271-a036daa43141",
   "metadata": {},
   "source": [
    "The noun chunks are contained in the `doc.noun_chunks` class variable. We can print out the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0e4f657-3b94-45a1-bd41-855ea8ad1bbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30566"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_noun_chunks = []\n",
    "\n",
    "for noun_chunk in doc.noun_chunks:\n",
    "    all_noun_chunks.append(noun_chunk.text)\n",
    "    \n",
    "len(all_noun_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d827794-b5c6-4419-a8eb-89929b307e44",
   "metadata": {},
   "source": [
    "#### How it worksâ€¦\n",
    "- The spaCy `Doc` object contains information about grammatical relationships between words in a sentence. \n",
    "    - Using this information, spaCy determines `noun phrases` or `chunks` contained in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fbe34681-4f45-4379-b4c7-b05fbdf635db",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"All emotions, and that one particularly, were abhorrent to his cold, precise but admirably balanced mind.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ffb25d0-1865-4a49-a427-a3cdc8830491",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "60aaec70-a9e6-42d9-8fe6-3be228521f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All emotions\n",
      "his cold, precise but admirably balanced mind\n"
     ]
    }
   ],
   "source": [
    "for noun_chunk in doc.noun_chunks:\n",
    "    print(noun_chunk.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3b374c07-020f-4a70-868b-16d71f87831f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the Column Names while initializing the Table\n",
    "nounTable = PrettyTable([\"Token\", \"Noun chunk start\", \"Noun chunk end\"])\n",
    "nounTable.align=\"l\"\n",
    "\n",
    "for noun_chunk in doc.noun_chunks:\n",
    "    nounTable.add_row([noun_chunk.text, noun_chunk.start, noun_chunk.end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "caf25ae8-ade8-4f9a-a179-5a7a34628fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------+------------------+----------------+\n",
      "| Token                                         | Noun chunk start | Noun chunk end |\n",
      "+-----------------------------------------------+------------------+----------------+\n",
      "| All emotions                                  | 0                | 2              |\n",
      "| his cold, precise but admirably balanced mind | 11               | 19             |\n",
      "+-----------------------------------------------+------------------+----------------+\n"
     ]
    }
   ],
   "source": [
    "print(nounTable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f2d6c9-8ed8-4a31-a9c8-48d372a46b25",
   "metadata": {},
   "source": [
    "Just like a sentence, any `noun chunk` includes a `root`, which is the token that all other tokens depend on. \n",
    "\n",
    "In a noun phrase, that is the `noun`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4fa2eaa3-6a26-4614-8af6-90dc9a593d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------+----------+\n",
      "|                     Token                     |   Root   |\n",
      "+-----------------------------------------------+----------+\n",
      "|                  All emotions                 | emotions |\n",
      "| his cold, precise but admirably balanced mind |   mind   |\n",
      "+-----------------------------------------------+----------+\n"
     ]
    }
   ],
   "source": [
    "# Specify the Column Names while initializing the Table\n",
    "noun_root_Table = PrettyTable([\"Token\", \"Root\"])\n",
    "\n",
    "for noun_chunk in doc.noun_chunks:\n",
    "    \n",
    "    noun_root_Table.add_row([noun_chunk.text, noun_chunk.root.text])\n",
    "\n",
    "print(noun_root_Table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d480e22-2fb7-4849-afd9-d5c7a01d5288",
   "metadata": {},
   "source": [
    "#### Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "069ad0cd-accb-4585-9a78-eaaf803bc45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_span = \"emotions\"\n",
    "other_doc = nlp(other_span)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739bb9ea-df11-40eb-8549-4210781347e7",
   "metadata": {},
   "source": [
    "We can now compare it to the noun chunks in the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4ebe23dd-3a3a-4e7b-a39f-ad3fa5f13d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------------------------------------+----------------------+\n",
      "| Token to compare |                   Noun chunk                  |   similarity score   |\n",
      "+------------------+-----------------------------------------------+----------------------+\n",
      "|     emotions     |                  All emotions                 |  0.4026422809451551  |\n",
      "|     emotions     | his cold, precise but admirably balanced mind | -0.03689126143699988 |\n",
      "+------------------+-----------------------------------------------+----------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhupe\\AppData\\Local\\Temp\\ipykernel_34348\\3016359861.py:6: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Span.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  sim_Table.add_row([other_span, noun_chunk.text, noun_chunk.similarity(other_doc)])\n"
     ]
    }
   ],
   "source": [
    "# Specify the Column Names while initializing the Table\n",
    "sim_Table = PrettyTable([\"Token to compare\", \"Noun chunk\", \"similarity score\"])\n",
    "\n",
    "for noun_chunk in doc.noun_chunks:\n",
    "    \n",
    "    sim_Table.add_row([other_span, noun_chunk.text, noun_chunk.similarity(other_doc)])\n",
    "    \n",
    "print(sim_Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cf8cc8-771b-4c68-8f91-39d186d26c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97fc77d-81bd-4ac0-9d04-c0ee0a594bd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
