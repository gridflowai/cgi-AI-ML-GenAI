{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e99a9595-4eb3-4f38-b0d2-5003372e695e",
   "metadata": {},
   "source": [
    "---------------\n",
    "#### Chunking and POS Tagging: \n",
    "- Breaking down text into chunks (like noun phrases) using Part-of-Speech (POS) tags and then filtering relevant chunks to serve as key phrases.\n",
    "---------------\n",
    "\n",
    "`POS Tagging`: Before chunking, we tag the words with their parts of speech (POS). This is done to understand the grammatical structure of the sentence and the role of each word.\n",
    "\n",
    "`Chunking`: Using the POS tags, we can define patterns to identify chunks in the text. For keyphrase extraction, noun phrase chunking is commonly used, as noun phrases often serve as the subjects of the text and are likely to be key entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f83ace81-e943-4867-a0af-b7c6603d2738",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.chunk import RegexpParser\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcaa8c54-576d-4ec5-aca4-b352dd19f74d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\bhupe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bhupe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\bhupe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('brown')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "957f2a49-607c-4789-9bd6-57c1b1d9c3bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the chunk grammar. \n",
    "# Here, a basic noun phrase grammar is used\n",
    "grammar = r\"\"\"\n",
    "    NP: {<DT|PP\\$>?<JJ>*<NN.*>+}   # chunk determiner/possessive, adjectives and noun\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69c23a4b-f61f-4ac3-bf8d-cc9e9600b8b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the chunk parser\n",
    "chunk_parser = RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5022133d-143a-4b98-88a0-18e518c0a6db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sample text\n",
    "text = \"The quick brown fox jumps over the lazy dog.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b4498db-c239-413a-b031-328912d7aec3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize and POS tag the text\n",
    "tokens = nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a4dbab9-8a95-468d-ba80-98ac5cb7899a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tagged_tokens = nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3517268-d9ab-48ec-95b3-2dc4f8bec197",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parse the tagged tokens using the chunk parser\n",
    "tree = chunk_parser.parse(tagged_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1472a5e3-4341-415d-b77c-cb46dff0fddb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The quick brown fox', 'the lazy dog']\n"
     ]
    }
   ],
   "source": [
    "# Extract the noun phrases as keyphrases\n",
    "keyphrases = [' '.join(leaf[0] for leaf in subtree.leaves())\n",
    "              for subtree in tree.subtrees(lambda t: t.label() == 'NP')]\n",
    "\n",
    "print(keyphrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12be1f8a-97e2-48f3-8532-c9bd5da0ec6a",
   "metadata": {},
   "source": [
    "- The RegexpParser is used to create a chunk parser using the provided grammar.\n",
    "\n",
    "- The grammar specifies that a noun phrase (NP) can be an optional determiner (DT) followed by any number of adjectives (JJ) and then one or more nouns (NN.*).\n",
    "\n",
    "- After parsing the POS-tagged tokens, we extract the chunks labeled as NP (noun phrases) and consider them as keyphrases.\n",
    "\n",
    "- The keyphrases extracted from the given sample text using the above code will be: ['The quick brown fox', 'the lazy dog']."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46360936-292a-42c8-b668-43515d405c0f",
   "metadata": {},
   "source": [
    "#### Let's use a section of the Brown corpus, \n",
    "- specifically from the news category, to demonstrate key phrase extraction through chunking and POS tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f55e99ee-c691-4e5c-9c93-16dca2acbb06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define chunk grammar for noun phrases\n",
    "grammar = r\"\"\"\n",
    "     NP: {<DT>?<JJ>*<NN>}        # Chunk sequences of DT, JJ, and NN tags as NP (noun phrases)\n",
    "     VP: {<VB.*><DT>?<JJ>*<NN>}  # Chunk sequences of VB (verbs) followed by DT, JJ, and NN tags as VP (verb phrases) \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad1c7652-27e1-4fa4-b5a1-32916a07f265",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the chunk parser\n",
    "chunk_parser = RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72f5354d-9b1a-4029-a77a-5467bf270908",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract a sample from the Brown corpus\n",
    "text = \" \".join(brown.words(categories='news')[:1500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0719574b-2884-4a0b-b4ec-f44e506cfd6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize and POS tag the text\n",
    "tokens = nltk.word_tokenize(text)\n",
    "tagged_tokens = nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5249aa23-80d0-434e-852e-3a411440f3ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The quick brown fox', 'the lazy dog']\n"
     ]
    }
   ],
   "source": [
    "# Extract noun phrases as keyphrases\n",
    "keyphrases = [' '.join(leaf[0] for leaf in subtree.leaves())\n",
    "              for subtree in tree.subtrees(lambda t: t.label() == 'NP')]\n",
    "\n",
    "print(keyphrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d41221-b060-4647-8850-ffed4b6a1b88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
