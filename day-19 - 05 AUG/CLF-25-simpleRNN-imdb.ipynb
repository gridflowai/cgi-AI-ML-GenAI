{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "## Text classification - using RNNs\n",
    "--------------------------\n",
    "\n",
    "trains a recurrent neural network on the IMDB large movie review dataset for sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters of Embedding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Arguments`\n",
    "    - `input_ dim` : int > 0. Size of the vocabulary, ie. 1 + maximum integer index occurring in the input data.\n",
    "    \n",
    "    - `output_dim`: int >= 0. Dimension of the dense embedding.\n",
    "    - `init`: name of initialization function for the weights of the layer (see: initializations), or alternatively, Theano function to use for weights initialization. `This parameter is only relevant if you don't pass a _weights_ argument`.\n",
    "\n",
    "    - `weights`: list of Numpy arrays to set as initial weights. The list should have 1 element, of shape (input_dim, output_dim).\n",
    "\n",
    "    - `W_regularizer`: instance of the regularizers module (eg. L1 or L2 regularization), applied to the embedding matrix.\n",
    "    - `W_constraint`: instance of the constraints module (eg. maxnorm, nonneg), applied to the embedding matrix.\n",
    "    \n",
    "mask_zero: Whether or not the input value 0 is a special \"padding\" value that should be masked out. This is useful for recurrent layers which may take variable length input. If this is True then all subsequent layers in the model need to support masking or an exception will be raised. If mask_zero is set to True, as a consequence, index 0 cannot be used in the vocabulary (input_dim should equal |vocabulary| + 2).\n",
    "input_length: Length of input sequences, when it is constant. This argument is required if you are going to connect Flatten then Dense layers upstream (without it, the shape of the dense outputs cannot be computed).\n",
    "dropout: float between 0 and 1. Fraction of the embeddings to drop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parameters of SimpleRNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Arguments`:\n",
    "\n",
    "    - `units`: Positive integer, dimensionality of the output space.\n",
    "    - `activation`: Activation function to use. Default: hyperbolic tangent (`tanh`). \n",
    "        - If you pass None, no activation is applied\n",
    "    (ie. \"linear\" activation: `a(x) = x`).\n",
    "    - `use_bias`: Boolean, (default `True`), whether the layer uses a bias vector.\n",
    "    - `kernel_initializer`: Initializer for the `kernel` weights matrix,\n",
    "        - used for the linear transformation of the inputs. Default:\n",
    "    `glorot_uniform`.\n",
    "    - `recurrent_initializer`: Initializer for the `recurrent_kernel`weights matrix, used for the linear transformation of the recurrent state. Default: `orthogonal`.\n",
    "    - `bias_initializer`: Initializer for the bias vector. Default: `zeros`.\n",
    "    - `kernel_regularizer`: Regularizer function applied to the `kernel` weights matrix. Default: `None`.\n",
    "    \n",
    "    - `recurrent_regularizer`: Regularizer function applied to the\n",
    "    `recurrent_kernel` weights matrix. Default: `None`.\n",
    "\n",
    "    - `bias_regularizer`: Regularizer function applied to the bias vector. Default: `None`.\n",
    "    - `activity_regularizer`: Regularizer function applied to the output of the layer (its \"activation\"). Default: `None`.\n",
    "    - `kernel_constraint`: Constraint function applied to the `kernel` weights matrix. Default: `None`.\n",
    "    - `recurrent_constraint`: Constraint function applied to the `recurrent_kernel` weights matrix.  Default: `None`.\n",
    "    - `bias_constraint`: Constraint function applied to the bias vector. Default: `None`.\n",
    "    - `dropout`: Float between 0 and 1. Fraction of the units to drop for the linear transformation of the inputs. Default: 0.\n",
    "    - recurrent_dropout: Float between 0 and 1. Fraction of the units to drop for the linear transformation of the recurrent state. Default: 0.\n",
    "    - `return_sequences`: Boolean. Whether to return the last output\n",
    "    in the output sequence, or the full sequence. Default: `False`.\n",
    "    - `return_state`: Boolean. Whether to return the last state     in addition to the output. Default: `False`\n",
    "    - `go_backwards`: Boolean (default False). If True, process the input sequence backwards and return the reversed sequence.\n",
    "\n",
    "`Call arguments`:\n",
    "\n",
    "    - inputs: A 3D tensor, with shape `[batch, timesteps, feature]`.\n",
    "    - mask: Binary tensor of shape `[batch, timesteps]` indicating whether a given timestep should be masked.\n",
    "    - training: Python boolean indicating whether the layer should behave in training mode or in inference mode. This argument is passed to the cell when calling it. This is only relevant if `dropout` or `recurrent_dropout` is used.\n",
    "    - initial_state: List of initial state tensors to be passed to the first call of the cell.\n",
    "\n",
    "Examples:\n",
    "\n",
    "```python\n",
    "inputs = np.random.random([32, 10, 8]).astype(np.float32)\n",
    "simple_rnn = tf.keras.layers.SimpleRNN(4)\n",
    "\n",
    "output = simple_rnn(inputs)  # The output has shape `[32, 4]`.\n",
    "\n",
    "simple_rnn = tf.keras.layers.SimpleRNN(\n",
    "    4, return_sequences=True, return_state=True)\n",
    "\n",
    "- whole_sequence_output has shape `[32, 10, 4]`.\n",
    "-  final_state has shape `[32, 4]`.\n",
    "whole_sequence_output, final_state = simple_rnn(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 32)          320000    \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (None, 32)                2080      \n",
      "=================================================================\n",
      "Total params: 322,080\n",
      "Trainable params: 322,080\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(10000, 32))\n",
    "model.add(SimpleRNN(32))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " it can return a 3D tensor of shape (batch_size, time_steps, output_features) which is the full sequences of successive outputs for each time steps by adding return_sequences=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 32)          320000    \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, None, 32)          2080      \n",
      "=================================================================\n",
      "Total params: 322,080\n",
      "Trainable params: 322,080\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(10000, 32))\n",
    "model.add(SimpleRNN(32,return_sequences=True))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did we get the number of parameters?\n",
    "\n",
    "- g, no. of FFNNs in a unit (RNN has 1, GRU has 3, LSTM has 4)\n",
    "- h, size of hidden units\n",
    "- i, dimension/size of input\n",
    "\n",
    "Since every FFNN has h(h+i) + h parameters, we have\n",
    "- num_params = g Ã— [h(h+i) + h]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2080"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = 1\n",
    "h = 32\n",
    "i = 32\n",
    "\n",
    "g * (h*(h+i) + h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 32)          320000    \n",
      "_________________________________________________________________\n",
      "simple_rnn_2 (SimpleRNN)     (None, None, 64)          6208      \n",
      "_________________________________________________________________\n",
      "simple_rnn_3 (SimpleRNN)     (None, None, 32)          3104      \n",
      "_________________________________________________________________\n",
      "simple_rnn_4 (SimpleRNN)     (None, None, 32)          2080      \n",
      "=================================================================\n",
      "Total params: 331,392\n",
      "Trainable params: 331,392\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(10000, 32))                #32*10,000\n",
    "model.add(SimpleRNN(64,return_sequences=True)) #(32+64+1)*64=6208\n",
    "model.add(SimpleRNN(32,return_sequences=True)) #(64+32+1)*32=3104\n",
    "model.add(SimpleRNN(32,return_sequences=True)) #(32+32+1)*32=2080\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load IBMD data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "maxlen = 500\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 2s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "C:\\Users\\Bhupen\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "C:\\Users\\Bhupen\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "(input_train, y_train), (input_test, y_test) = imdb.load_data( num_words=max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n"
     ]
    }
   ],
   "source": [
    "print(len(input_train), 'train sequences')\n",
    "print(len(input_test), 'test sequences')\n",
    "print('Pad sequences (samples x time)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_train shape: (25000, 500)\n",
      "input_test shape: (25000, 500)\n"
     ]
    }
   ],
   "source": [
    "input_train = sequence.pad_sequences(input_train, maxlen=maxlen)\n",
    "input_test  = sequence.pad_sequences(input_test,  maxlen=maxlen)\n",
    "\n",
    "print('input_train shape:', input_train.shape)\n",
    "print('input_test shape:', input_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 32)          320000    \n",
      "_________________________________________________________________\n",
      "simple_rnn_5 (SimpleRNN)     (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 322,113\n",
      "Trainable params: 322,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32)) #max_feature=10,000 so, 320,000\n",
    "model.add(SimpleRNN(32))               #(32+32+1)*32=2080\n",
    "model.add(Dense(1, activation='sigmoid'))#(32+1)*1=33\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "157/157 [==============================] - 18s 107ms/step - loss: 0.6422 - acc: 0.6061 - val_loss: 0.4010 - val_acc: 0.8300\n",
      "Epoch 2/10\n",
      "157/157 [==============================] - 17s 108ms/step - loss: 0.3599 - acc: 0.8541 - val_loss: 0.3570 - val_acc: 0.8584\n",
      "Epoch 3/10\n",
      "157/157 [==============================] - 17s 106ms/step - loss: 0.2760 - acc: 0.8943 - val_loss: 0.3820 - val_acc: 0.8564\n",
      "Epoch 4/10\n",
      "157/157 [==============================] - 17s 107ms/step - loss: 0.4480 - acc: 0.8437 - val_loss: 0.3217 - val_acc: 0.8638\n",
      "Epoch 5/10\n",
      "157/157 [==============================] - 18s 114ms/step - loss: 0.2067 - acc: 0.9263 - val_loss: 0.3957 - val_acc: 0.8452\n",
      "Epoch 6/10\n",
      "157/157 [==============================] - 17s 106ms/step - loss: 0.1704 - acc: 0.9361 - val_loss: 0.4401 - val_acc: 0.8570\n",
      "Epoch 7/10\n",
      "157/157 [==============================] - 17s 107ms/step - loss: 0.1391 - acc: 0.9535 - val_loss: 0.4008 - val_acc: 0.8506\n",
      "Epoch 8/10\n",
      "157/157 [==============================] - 17s 107ms/step - loss: 0.1035 - acc: 0.9653 - val_loss: 0.4301 - val_acc: 0.8312\n",
      "Epoch 9/10\n",
      "157/157 [==============================] - 18s 113ms/step - loss: 0.1422 - acc: 0.9573 - val_loss: 0.5273 - val_acc: 0.7916\n",
      "Epoch 10/10\n",
      "157/157 [==============================] - 18s 118ms/step - loss: 0.0638 - acc: 0.9795 - val_loss: 0.4916 - val_acc: 0.8488\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy',metrics=['acc'])\n",
    "\n",
    "history = model.fit(input_train, y_train,epochs=10, batch_size=128, validation_split=0.2)\n",
    "\n",
    "#25,000*0.8=20,000 (train on 20000samples) 5000 left for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
